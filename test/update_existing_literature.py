#!/usr/bin/env python3
"""
æ•°æ®åº“æ›´æ–°è„šæœ¬

ä¸ºç°æœ‰æ–‡çŒ®ç”Ÿæˆæ–‡æœ¬å—å’Œembeddingï¼Œæ‰¹é‡å¤„ç†å·²ä¸Šä¼ çš„æ–‡çŒ®
"""
import sys
import os
import logging
import asyncio
from datetime import datetime
from typing import List, Dict, Optional
from sqlalchemy.orm import Session

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from app.database import get_db
from app.models.literature import Literature
from app.models.research_group import ResearchGroup
from app.utils.async_processor import AsyncProcessor
from app.utils.vector_store import vector_store
from app.utils.embedding_service import embedding_service
from app.utils.text_extractor import extract_text_from_file
from app.utils.text_processor import split_text_into_chunks
from app.utils.error_handler import log_error, log_success

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('update_literature.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class LiteratureUpdater:
    """æ–‡çŒ®æ›´æ–°å™¨"""
    
    def __init__(self):
        self.processed_count = 0
        self.success_count = 0
        self.error_count = 0
        self.skipped_count = 0
        self.total_count = 0
        self.processor = AsyncProcessor()
        
    def get_literature_list(self, db: Session, group_id: Optional[str] = None, 
                           limit: Optional[int] = None) -> List[Literature]:
        """è·å–éœ€è¦å¤„ç†çš„æ–‡çŒ®åˆ—è¡¨"""
        try:
            query = db.query(Literature).filter(Literature.status == "active")
            
            if group_id:
                query = query.filter(Literature.group_id == group_id)
            
            if limit:
                query = query.limit(limit)
            
            literature_list = query.all()
            logger.info(f"æ‰¾åˆ° {len(literature_list)} ä¸ªéœ€è¦å¤„ç†çš„æ–‡çŒ®")
            return literature_list
            
        except Exception as e:
            logger.error(f"è·å–æ–‡çŒ®åˆ—è¡¨å¤±è´¥: {e}")
            return []
    
    def check_literature_processed(self, literature_id: str, group_id: str) -> bool:
        """æ£€æŸ¥æ–‡çŒ®æ˜¯å¦å·²ç»å¤„ç†è¿‡"""
        try:
            # æ£€æŸ¥å‘é‡æ•°æ®åº“ä¸­æ˜¯å¦å·²æœ‰è¯¥æ–‡çŒ®çš„æ•°æ®
            chunks = vector_store.get_literature_chunks(literature_id, group_id)
            return len(chunks) > 0
        except:
            return False
    
    async def process_single_literature(self, literature: Literature, 
                                      force_update: bool = False) -> bool:
        """å¤„ç†å•ä¸ªæ–‡çŒ®"""
        literature_id = literature.id
        group_id = literature.group_id
        file_path = literature.file_path
        
        try:
            logger.info(f"å¼€å§‹å¤„ç†æ–‡çŒ®: {literature.title} (ID: {literature_id})")
            
            # æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
            if not force_update and self.check_literature_processed(literature_id, group_id):
                logger.info(f"æ–‡çŒ® {literature_id} å·²å¤„ç†è¿‡ï¼Œè·³è¿‡")
                self.skipped_count += 1
                return True
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(file_path):
                logger.error(f"æ–‡çŒ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                self.error_count += 1
                return False
            
            # 1. æå–æ–‡æœ¬
            logger.info(f"æå–æ–‡æœ¬: {file_path}")
            text_content = extract_text_from_file(file_path)
            
            if not text_content or len(text_content.strip()) < 100:
                logger.warning(f"æ–‡çŒ® {literature_id} æ–‡æœ¬å†…å®¹å¤ªå°‘æˆ–ä¸ºç©ºï¼Œè·³è¿‡")
                self.skipped_count += 1
                return True
            
            # 2. æ–‡æœ¬åˆ†å—
            logger.info(f"æ–‡æœ¬åˆ†å—: {len(text_content)} å­—ç¬¦")
            chunks = split_text_into_chunks(
                text_content, 
                chunk_size=1000, 
                overlap=200
            )
            
            if not chunks:
                logger.warning(f"æ–‡çŒ® {literature_id} åˆ†å—å¤±è´¥ï¼Œè·³è¿‡")
                self.skipped_count += 1
                return True
            
            logger.info(f"ç”Ÿæˆäº† {len(chunks)} ä¸ªæ–‡æœ¬å—")
            
            # 3. ç”Ÿæˆembeddings
            logger.info("ç”Ÿæˆembeddings...")
            embeddings = []
            failed_chunks = []
            
            for i, chunk in enumerate(chunks):
                try:
                    embedding = embedding_service.generate_embedding(chunk)
                    if embedding:
                        embeddings.append(embedding)
                    else:
                        failed_chunks.append(i)
                        logger.warning(f"æ–‡æ¡£å— {i} embeddingç”Ÿæˆå¤±è´¥")
                except Exception as e:
                    failed_chunks.append(i)
                    logger.warning(f"æ–‡æ¡£å— {i} embeddingç”Ÿæˆå¼‚å¸¸: {e}")
                
                # æ·»åŠ å°å»¶è¿Ÿé¿å…APIé™åˆ¶
                if i % 10 == 0:
                    await asyncio.sleep(0.1)
            
            if not embeddings:
                logger.error(f"æ–‡çŒ® {literature_id} æ‰€æœ‰embeddingç”Ÿæˆéƒ½å¤±è´¥")
                self.error_count += 1
                return False
            
            # ç§»é™¤å¤±è´¥çš„å—
            if failed_chunks:
                chunks = [chunk for i, chunk in enumerate(chunks) if i not in failed_chunks]
                logger.info(f"ç§»é™¤ {len(failed_chunks)} ä¸ªå¤±è´¥çš„å—ï¼Œå‰©ä½™ {len(chunks)} ä¸ª")
            
            # 4. å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
            logger.info("å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“...")
            
            # å¦‚æœå¼ºåˆ¶æ›´æ–°ï¼Œå…ˆåˆ é™¤ç°æœ‰æ•°æ®
            if force_update:
                vector_store.delete_literature_chunks(literature_id, group_id)
                logger.info(f"å·²åˆ é™¤æ–‡çŒ® {literature_id} çš„ç°æœ‰å‘é‡æ•°æ®")
            
            # å‡†å¤‡æ–‡æ¡£å—æ•°æ®
            chunk_data_list = []
            for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
                chunk_data = {
                    "text": chunk,
                    "literature_id": literature_id,
                    "group_id": group_id,
                    "chunk_index": i,
                    "metadata": {
                        "source": literature.title,
                        "file_path": file_path,
                        "chunk_length": len(chunk),
                        "processed_at": datetime.now().isoformat()
                    }
                }
                chunk_data_list.append(chunk_data)
            
            # æ‰¹é‡å­˜å‚¨
            success = await vector_store.store_document_chunks_async(
                chunks=chunk_data_list,
                embeddings=embeddings,
                literature_id=literature_id,
                group_id=group_id
            )
            
            if success:
                logger.info(f"âœ… æ–‡çŒ® {literature_id} å¤„ç†å®Œæˆ: {len(chunks)} ä¸ªå—")
                self.success_count += 1
                
                # è®°å½•æˆåŠŸæ—¥å¿—
                log_success("literature_processing", literature_id, {
                    "title": literature.title,
                    "chunks_count": len(chunks),
                    "embeddings_count": len(embeddings),
                    "text_length": len(text_content)
                })
                return True
            else:
                logger.error(f"âŒ æ–‡çŒ® {literature_id} å‘é‡å­˜å‚¨å¤±è´¥")
                self.error_count += 1
                return False
                
        except Exception as e:
            logger.error(f"âŒ å¤„ç†æ–‡çŒ® {literature_id} æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
            log_error("literature_processing", e, extra_info={
                "literature_id": literature_id,
                "title": literature.title,
                "file_path": file_path
            })
            self.error_count += 1
            return False
        finally:
            self.processed_count += 1
    
    async def update_literature_batch(self, db: Session, group_id: Optional[str] = None,
                                    limit: Optional[int] = None, 
                                    force_update: bool = False,
                                    max_concurrent: int = 3) -> Dict:
        """æ‰¹é‡æ›´æ–°æ–‡çŒ®"""
        start_time = datetime.now()
        logger.info("ğŸš€ å¼€å§‹æ‰¹é‡æ›´æ–°æ–‡çŒ®...")
        
        # è·å–æ–‡çŒ®åˆ—è¡¨
        literature_list = self.get_literature_list(db, group_id, limit)
        self.total_count = len(literature_list)
        
        if self.total_count == 0:
            logger.warning("æ²¡æœ‰æ‰¾åˆ°éœ€è¦å¤„ç†çš„æ–‡çŒ®")
            return self._get_summary(start_time)
        
        logger.info(f"å‡†å¤‡å¤„ç† {self.total_count} ä¸ªæ–‡çŒ® (æœ€å¤§å¹¶å‘: {max_concurrent})")
        
        # åˆ›å»ºä¿¡å·é‡é™åˆ¶å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def process_with_semaphore(lit):
            async with semaphore:
                return await self.process_single_literature(lit, force_update)
        
        # æ‰¹é‡å¤„ç†
        tasks = [process_with_semaphore(lit) for lit in literature_list]
        
        # æ˜¾ç¤ºè¿›åº¦
        for i, task in enumerate(asyncio.as_completed(tasks)):
            try:
                await task
                progress = (i + 1) / self.total_count * 100
                logger.info(f"è¿›åº¦: {i+1}/{self.total_count} ({progress:.1f}%)")
            except Exception as e:
                logger.error(f"ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸: {e}")
        
        return self._get_summary(start_time)
    
    def _get_summary(self, start_time: datetime) -> Dict:
        """è·å–å¤„ç†æ€»ç»“"""
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        summary = {
            "start_time": start_time.isoformat(),
            "end_time": end_time.isoformat(),
            "duration_seconds": duration,
            "total_literature": self.total_count,
            "processed": self.processed_count,
            "success": self.success_count,
            "errors": self.error_count,
            "skipped": self.skipped_count,
            "success_rate": (self.success_count / self.total_count * 100) if self.total_count > 0 else 0
        }
        
        logger.info("=" * 60)
        logger.info("ğŸ“Š å¤„ç†æ€»ç»“")
        logger.info("=" * 60)
        logger.info(f"æ€»æ–‡çŒ®æ•°: {summary['total_literature']}")
        logger.info(f"å·²å¤„ç†: {summary['processed']}")
        logger.info(f"æˆåŠŸ: {summary['success']}")
        logger.info(f"å¤±è´¥: {summary['errors']}")
        logger.info(f"è·³è¿‡: {summary['skipped']}")
        logger.info(f"æˆåŠŸç‡: {summary['success_rate']:.1f}%")
        logger.info(f"æ€»è€—æ—¶: {summary['duration_seconds']:.1f}ç§’")
        
        return summary
    
    async def verify_processing_results(self, db: Session, group_id: Optional[str] = None) -> Dict:
        """éªŒè¯å¤„ç†ç»“æœ"""
        logger.info("ğŸ” éªŒè¯å¤„ç†ç»“æœ...")
        
        literature_list = self.get_literature_list(db, group_id)
        verification_results = {
            "total_literature": len(literature_list),
            "processed_literature": 0,
            "unprocessed_literature": 0,
            "vector_stats": {},
            "details": []
        }
        
        for literature in literature_list:
            try:
                # æ£€æŸ¥å‘é‡æ•°æ®åº“ä¸­çš„æ•°æ®
                chunks = vector_store.get_literature_chunks(
                    literature.id, literature.group_id
                )
                
                is_processed = len(chunks) > 0
                if is_processed:
                    verification_results["processed_literature"] += 1
                else:
                    verification_results["unprocessed_literature"] += 1
                
                verification_results["details"].append({
                    "literature_id": literature.id,
                    "title": literature.title,
                    "is_processed": is_processed,
                    "chunks_count": len(chunks)
                })
                
            except Exception as e:
                logger.error(f"éªŒè¯æ–‡çŒ® {literature.id} æ—¶å‡ºé”™: {e}")
                verification_results["details"].append({
                    "literature_id": literature.id,
                    "title": literature.title,
                    "is_processed": False,
                    "error": str(e)
                })
        
        # è·å–å‘é‡æ•°æ®åº“ç»Ÿè®¡
        try:
            verification_results["vector_stats"] = vector_store.get_stats()
        except Exception as e:
            logger.error(f"è·å–å‘é‡æ•°æ®åº“ç»Ÿè®¡å¤±è´¥: {e}")
        
        logger.info(f"éªŒè¯å®Œæˆ: {verification_results['processed_literature']}/{verification_results['total_literature']} ä¸ªæ–‡çŒ®å·²å¤„ç†")
        
        return verification_results

async def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="æ›´æ–°ç°æœ‰æ–‡çŒ®çš„å‘é‡æ•°æ®")
    parser.add_argument("--group-id", help="æŒ‡å®šç ”ç©¶ç»„ID (å¯é€‰)")
    parser.add_argument("--limit", type=int, help="é™åˆ¶å¤„ç†æ•°é‡ (å¯é€‰)")
    parser.add_argument("--force", action="store_true", help="å¼ºåˆ¶æ›´æ–°å·²å¤„ç†çš„æ–‡çŒ®")
    parser.add_argument("--verify-only", action="store_true", help="åªéªŒè¯ä¸å¤„ç†")
    parser.add_argument("--max-concurrent", type=int, default=3, help="æœ€å¤§å¹¶å‘æ•°")
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
    db_gen = get_db()
    db = next(db_gen)
    
    try:
        updater = LiteratureUpdater()
        
        if args.verify_only:
            # åªéªŒè¯
            verification_results = await updater.verify_processing_results(db, args.group_id)
            
            # ä¿å­˜éªŒè¯ç»“æœ
            import json
            with open('verification_results.json', 'w', encoding='utf-8') as f:
                json.dump(verification_results, f, ensure_ascii=False, indent=2)
            
            logger.info("éªŒè¯ç»“æœå·²ä¿å­˜åˆ°: verification_results.json")
        else:
            # æ‰§è¡Œæ›´æ–°
            summary = await updater.update_literature_batch(
                db=db,
                group_id=args.group_id,
                limit=args.limit,
                force_update=args.force,
                max_concurrent=args.max_concurrent
            )
            
            # ä¿å­˜å¤„ç†ç»“æœ
            import json
            with open('update_summary.json', 'w', encoding='utf-8') as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)
            
            logger.info("å¤„ç†ç»“æœå·²ä¿å­˜åˆ°: update_summary.json")
            
            # å¦‚æœæœ‰é”™è¯¯ï¼Œé€€å‡ºæ—¶è¿”å›é”™è¯¯ç 
            if summary['errors'] > 0:
                logger.warning("éƒ¨åˆ†æ–‡çŒ®å¤„ç†å¤±è´¥ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—")
                sys.exit(1)
            else:
                logger.info("ğŸ‰ æ‰€æœ‰æ–‡çŒ®å¤„ç†å®Œæˆï¼")
                sys.exit(0)
        
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­äº†å¤„ç†è¿‡ç¨‹")
        sys.exit(1)
    except Exception as e:
        logger.error(f"æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {e}")
        sys.exit(1)
    finally:
        db.close()

if __name__ == "__main__":
    asyncio.run(main()) 