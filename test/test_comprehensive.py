#!/usr/bin/env python3
"""
ç»¼åˆåŠŸèƒ½æµ‹è¯•è„šæœ¬
æµ‹è¯•æ–‡çŒ®ç®¡ç†ç³»ç»Ÿçš„æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½
"""

import os
import sys
import tempfile
import time
from pathlib import Path
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

def test_google_api():
    """æµ‹è¯•Google APIè¿æ¥"""
    print("ğŸŒ æµ‹è¯•Google APIè¿æ¥...")
    
    try:
        from google import genai
        from google.genai import types
        
        api_key = os.getenv("GOOGLE_API_KEY")
        if not api_key:
            print("âŒ Google APIå¯†é’¥æœªé…ç½®")
            return False
        
        client = genai.Client(api_key=api_key)
        
        # æµ‹è¯•embeddingç”Ÿæˆ
        response = client.models.embed_content(
            model="text-embedding-004",
            contents="æµ‹è¯•æ–‡æœ¬",
            config=types.EmbedContentConfig(
                task_type="RETRIEVAL_DOCUMENT"
            )
        )
        
        if response.embeddings and len(response.embeddings) > 0:
            embedding_dim = len(response.embeddings[0].values)
            print(f"âœ… Google APIè¿æ¥æ­£å¸¸ï¼Œembeddingç»´åº¦: {embedding_dim}")
            return True
        else:
            print("âŒ Google APIå“åº”å¼‚å¸¸")
            return False
            
    except Exception as e:
        print(f"âŒ Google APIè¿æ¥å¤±è´¥: {e}")
        return False

def test_embedding_service():
    """æµ‹è¯•embeddingæœåŠ¡"""
    print("\nğŸ§  æµ‹è¯•EmbeddingæœåŠ¡...")
    
    try:
        sys.path.append('.')
        from app.utils.embedding_service import embedding_service
        
        # æµ‹è¯•è¿æ¥
        result = embedding_service.test_connection()
        if result.get('status') == 'success':
            print(f"âœ… EmbeddingæœåŠ¡å¯ç”¨: {result['provider']}")
            print(f"   æ¨¡å‹: {result['model']}")
            print(f"   ç»´åº¦: {result['embedding_dimension']}")
            print(f"   å“åº”æ—¶é—´: {result['response_time_seconds']}ç§’")
            
            # æµ‹è¯•æ‰¹é‡ç”Ÿæˆ
            test_texts = [
                "äººå·¥æ™ºèƒ½æŠ€æœ¯å‘å±•è¿…é€Ÿ",
                "æœºå™¨å­¦ä¹ æ˜¯AIçš„é‡è¦åˆ†æ”¯",
                "æ·±åº¦å­¦ä¹ åœ¨å›¾åƒè¯†åˆ«ä¸­è¡¨ç°ä¼˜å¼‚"
            ]
            
            embeddings, failed = embedding_service.batch_generate_embeddings(test_texts, batch_size=2)
            print(f"âœ… æ‰¹é‡embeddingç”Ÿæˆ: {len(embeddings)}/{len(test_texts)} æˆåŠŸ")
            
            return True
        else:
            print(f"âŒ EmbeddingæœåŠ¡ä¸å¯ç”¨: {result.get('error')}")
            return False
            
    except Exception as e:
        print(f"âŒ EmbeddingæœåŠ¡æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_vector_store():
    """æµ‹è¯•å‘é‡æ•°æ®åº“"""
    print("\nğŸ—„ï¸  æµ‹è¯•å‘é‡æ•°æ®åº“...")
    
    try:
        sys.path.append('.')
        from app.utils.vector_store import VectorStore
        
        # åˆ›å»ºæµ‹è¯•å®ä¾‹
        vector_store = VectorStore()
        
        # æµ‹è¯•å¥åº·æ£€æŸ¥
        health = vector_store.health_check()
        print(f"âœ… å‘é‡æ•°æ®åº“å¥åº·çŠ¶æ€: {health['status']}")
        print(f"   é›†åˆæ•°é‡: {health['collections_count']}")
        
        # æµ‹è¯•é›†åˆæ“ä½œ
        test_group_id = "test_group_comprehensive"
        collection = vector_store.get_or_create_collection(test_group_id)
        print(f"âœ… é›†åˆæ“ä½œæˆåŠŸ: {collection.name}")
        
        # æµ‹è¯•æ–‡æ¡£å­˜å‚¨
        test_chunks = [
            {
                'text': 'äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„åˆ†æ”¯ï¼Œç ”ç©¶å¦‚ä½•è®©æœºå™¨æ¨¡æ‹Ÿäººç±»æ™ºèƒ½',
                'chunk_index': 0,
                'literature_id': 'test_lit_001',
                'group_id': test_group_id,
                'literature_title': 'äººå·¥æ™ºèƒ½æ¦‚è®º',
                'chunk_length': 30,
                'chunk_id': 'test_lit_001_chunk_0'
            },
            {
                'text': 'æœºå™¨å­¦ä¹ ä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ ï¼Œæ— éœ€æ˜ç¡®ç¼–ç¨‹',
                'chunk_index': 1,
                'literature_id': 'test_lit_001',
                'group_id': test_group_id,
                'literature_title': 'äººå·¥æ™ºèƒ½æ¦‚è®º',
                'chunk_length': 24,
                'chunk_id': 'test_lit_001_chunk_1'
            }
        ]
        
        # ä¿®æ­£æ–¹æ³•è°ƒç”¨ - ä½¿ç”¨æ–°çš„ä¾¿æ·æ–¹æ³•
        success = vector_store.store_document_chunks_with_embeddings(test_chunks, 'test_lit_001', test_group_id)
        print(f"âœ… æ–‡æ¡£å­˜å‚¨æˆåŠŸ: {success}")
        
        # æµ‹è¯•æœç´¢ - ä½¿ç”¨æ–°çš„ä¾¿æ·æ–¹æ³•
        results = vector_store.search_similar_chunks_by_query(
            query="ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ", 
            group_id=test_group_id, 
            top_k=2
        )
        print(f"âœ… ç›¸ä¼¼åº¦æœç´¢: æ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
        for i, result in enumerate(results, 1):
            print(f"   ç»“æœ{i}: ç›¸ä¼¼åº¦ {result['similarity']:.3f}")
            print(f"         æ–‡æœ¬: {result['text'][:50]}...")
        
        # æ¸…ç†æµ‹è¯•æ•°æ®
        vector_store.delete_document_chunks('test_lit_001', test_group_id)
        print("âœ… æµ‹è¯•æ•°æ®æ¸…ç†å®Œæˆ")
        
        return True
        
    except Exception as e:
        print(f"âŒ å‘é‡æ•°æ®åº“æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_text_processing():
    """æµ‹è¯•æ–‡æœ¬å¤„ç†åŠŸèƒ½"""
    print("\nğŸ“ æµ‹è¯•æ–‡æœ¬å¤„ç†åŠŸèƒ½...")
    
    try:
        sys.path.append('.')
        from app.utils.text_processor import (
            split_text_into_chunks, 
            prepare_chunks_for_embedding, 
            estimate_token_count
        )
        from app.utils.text_extractor import clean_extracted_text
        
        # æµ‹è¯•æ–‡æœ¬æ¸…ç†
        dirty_text = "   è¿™æ˜¯ä¸€ä¸ª   æµ‹è¯•æ–‡æœ¬\n\n\n   åŒ…å«å¤šä½™ç©ºç™½   "
        clean_text = clean_extracted_text(dirty_text)
        print(f"âœ… æ–‡æœ¬æ¸…ç†: '{dirty_text}' -> '{clean_text}'")
        
        # æµ‹è¯•æ–‡æœ¬åˆ†å—
        long_text = """
        äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligenceï¼ŒAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä¼å›¾äº†è§£æ™ºèƒ½çš„å®è´¨ï¼Œ
        å¹¶ç”Ÿäº§å‡ºä¸€ç§æ–°çš„èƒ½ä»¥äººç±»æ™ºèƒ½ç›¸ä¼¼çš„æ–¹å¼åšå‡ºååº”çš„æ™ºèƒ½æœºå™¨ã€‚æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦
        åˆ†æ”¯ï¼Œå®ƒæ˜¯ä¸€ç§é€šè¿‡ç®—æ³•ä½¿æœºå™¨èƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ å¹¶åšå‡ºå†³ç­–æˆ–é¢„æµ‹çš„æŠ€æœ¯ã€‚æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ 
        çš„ä¸€ä¸ªå­é›†ï¼Œå®ƒä½¿ç”¨ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å·¥ä½œæ–¹å¼ã€‚
        
        è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNatural Language Processingï¼ŒNLPï¼‰æ˜¯äººå·¥æ™ºèƒ½å’Œè¯­è¨€å­¦é¢†åŸŸçš„åˆ†æ”¯å­¦ç§‘ã€‚
        æ­¤é¢†åŸŸæ¢è®¨å¦‚ä½•å¤„ç†åŠè¿ç”¨è‡ªç„¶è¯­è¨€ï¼›è‡ªç„¶è¯­è¨€å¤„ç†åŒ…æ‹¬å¤šä¸ªæ­¥éª¤ï¼šè¯æ³•åˆ†æã€è¯­æ³•åˆ†æã€
        è¯­ä¹‰åˆ†æã€è¯­ç”¨åˆ†æç­‰ã€‚æœºå™¨ç¿»è¯‘ï¼Œè¯­éŸ³è¯†åˆ«ï¼Œæ–‡æœ¬åˆ†ç±»éƒ½æ˜¯è‡ªç„¶è¯­è¨€å¤„ç†çš„åº”ç”¨ã€‚
        """
        
        chunks = split_text_into_chunks(long_text, chunk_size=200, overlap=50)
        print(f"âœ… æ–‡æœ¬åˆ†å—: {len(long_text)} å­—ç¬¦ -> {len(chunks)} ä¸ªå—")
        
        # æµ‹è¯•embeddingæ•°æ®å‡†å¤‡
        chunk_data = prepare_chunks_for_embedding(
            chunks, 
            literature_id="test_lit_processing",
            group_id="test_group_processing",
            literature_title="æ–‡æœ¬å¤„ç†æµ‹è¯•"
        )
        print(f"âœ… Embeddingæ•°æ®å‡†å¤‡: {len(chunk_data)} ä¸ªæ•°æ®å—")
        
        # æµ‹è¯•tokenè®¡ç®—
        token_count = estimate_token_count(long_text, model_type="google")
        print(f"âœ… Tokenä¼°ç®—: {len(long_text)} å­—ç¬¦ â‰ˆ {token_count} tokens")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ–‡æœ¬å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_file_upload_processing():
    """æµ‹è¯•æ–‡ä»¶ä¸Šä¼ å’Œå¤„ç†"""
    print("\nğŸ“ æµ‹è¯•æ–‡ä»¶å¤„ç†åŠŸèƒ½...")
    
    try:
        sys.path.append('.')
        from app.utils.storage_manager import StorageManager
        from app.utils.text_extractor import extract_text_from_file
        
        # åˆ›å»ºä¸´æ—¶æµ‹è¯•æ–‡ä»¶
        test_content = """
        # æµ‹è¯•æ–‡æ¡£
        
        è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£ï¼Œç”¨äºéªŒè¯æ–‡ä»¶å¤„ç†åŠŸèƒ½ã€‚
        
        ## ç¬¬ä¸€ç« ï¼šäººå·¥æ™ºèƒ½ç®€ä»‹
        
        äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯ä¸€ä¸ªè·¨å­¦ç§‘çš„ç§‘å­¦é¢†åŸŸï¼Œè‡´åŠ›äºç†è§£å’Œæ„å»ºæ™ºèƒ½è¡Œä¸ºã€‚
        å®ƒç»“åˆäº†è®¡ç®—æœºç§‘å­¦ã€æ•°å­¦ã€å¿ƒç†å­¦ã€è¯­è¨€å­¦ç­‰å¤šä¸ªå­¦ç§‘ã€‚
        
        ## ç¬¬äºŒç« ï¼šæœºå™¨å­¦ä¹ 
        
        æœºå™¨å­¦ä¹ æ˜¯AIçš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œé€šè¿‡ç®—æ³•è®©è®¡ç®—æœºä»æ•°æ®ä¸­å­¦ä¹ æ¨¡å¼ã€‚
        å¸¸è§çš„æœºå™¨å­¦ä¹ æ–¹æ³•åŒ…æ‹¬ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ã€‚
        
        ## ç»“è®º
        
        AIæŠ€æœ¯æ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œåœ¨å„ä¸ªé¢†åŸŸéƒ½æœ‰å¹¿æ³›åº”ç”¨ã€‚
        """
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False, encoding='utf-8') as f:
            f.write(test_content)
            temp_file_path = f.name
        
        try:
            # æµ‹è¯•æ–‡æœ¬æå–
            extracted_text = extract_text_from_file(temp_file_path)
            print(f"âœ… æ–‡æœ¬æå–: {len(extracted_text)} å­—ç¬¦")
            
            # æµ‹è¯•å­˜å‚¨ç®¡ç†
            storage = StorageManager()
            test_group_id = "test_group_storage"
            
            # æ¨¡æ‹Ÿæ–‡ä»¶ä¸Šä¼ 
            with open(temp_file_path, 'rb') as f:
                file_data = f.read()
            
            # è¿™é‡Œç®€åŒ–æµ‹è¯•ï¼Œä¸å®é™…ä¿å­˜æ–‡ä»¶
            print("âœ… æ–‡ä»¶è¯»å–æˆåŠŸ")
            print(f"   æ–‡ä»¶å¤§å°: {len(file_data)} å­—èŠ‚")
            
            return True
            
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.unlink(temp_file_path)
            
    except Exception as e:
        print(f"âŒ æ–‡ä»¶å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_end_to_end_workflow():
    """æµ‹è¯•ç«¯åˆ°ç«¯å·¥ä½œæµç¨‹"""
    print("\nğŸ”„ æµ‹è¯•ç«¯åˆ°ç«¯å·¥ä½œæµç¨‹...")
    
    try:
        sys.path.append('.')
        from app.utils.text_processor import split_text_into_chunks, prepare_chunks_for_embedding
        from app.utils.embedding_service import embedding_service
        from app.utils.vector_store import VectorStore
        
        # æ¨¡æ‹Ÿæ–‡çŒ®å¤„ç†æµç¨‹
        test_text = """
        æœºå™¨å­¦ä¹ ç®—æ³•åˆ†æï¼š
        
        1. ç›‘ç£å­¦ä¹ ï¼šä½¿ç”¨æ ‡è®°æ•°æ®è®­ç»ƒæ¨¡å‹ï¼Œå¦‚åˆ†ç±»å’Œå›å½’é—®é¢˜ã€‚
           å¸¸è§ç®—æ³•ï¼šçº¿æ€§å›å½’ã€å†³ç­–æ ‘ã€éšæœºæ£®æ—ã€æ”¯æŒå‘é‡æœºã€‚
        
        2. æ— ç›‘ç£å­¦ä¹ ï¼šä»æœªæ ‡è®°æ•°æ®ä¸­å‘ç°æ¨¡å¼ã€‚
           å¸¸è§ç®—æ³•ï¼šèšç±»ã€é™ç»´ã€å…³è”è§„åˆ™æŒ–æ˜ã€‚
        
        3. å¼ºåŒ–å­¦ä¹ ï¼šé€šè¿‡ä¸ç¯å¢ƒäº¤äº’å­¦ä¹ æœ€ä¼˜ç­–ç•¥ã€‚
           åº”ç”¨é¢†åŸŸï¼šæ¸¸æˆAIã€æœºå™¨äººæ§åˆ¶ã€æ¨èç³»ç»Ÿã€‚
        
        é€‰æ‹©åˆé€‚çš„ç®—æ³•éœ€è¦è€ƒè™‘æ•°æ®ç‰¹å¾ã€é—®é¢˜ç±»å‹å’Œè®¡ç®—èµ„æºã€‚
        """
        
        # æ­¥éª¤1: æ–‡æœ¬å¤„ç†
        chunks = split_text_into_chunks(test_text, chunk_size=150, overlap=30)
        chunk_data = prepare_chunks_for_embedding(
            chunks,
            literature_id="test_workflow_lit",
            group_id="test_workflow_group",
            literature_title="æœºå™¨å­¦ä¹ ç®—æ³•ç»¼è¿°"
        )
        print(f"âœ… æ­¥éª¤1 - æ–‡æœ¬å¤„ç†: {len(chunk_data)} ä¸ªå—")
        
        # æ­¥éª¤2: ç”Ÿæˆembeddings
        embeddings, failed = embedding_service.batch_generate_embeddings(
            [chunk['text'] for chunk in chunk_data]
        )
        print(f"âœ… æ­¥éª¤2 - Embeddingç”Ÿæˆ: {len(embeddings)} ä¸ªå‘é‡")
        
        # æ­¥éª¤3: å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
        if embeddings:
            vector_store = VectorStore()
            success = vector_store.store_document_chunks_with_embeddings(chunk_data, "test_workflow_lit", "test_workflow_group")
            print(f"âœ… æ­¥éª¤3 - å‘é‡å­˜å‚¨: {'æˆåŠŸ' if success else 'å¤±è´¥'}")
            
            # æ­¥éª¤4: æµ‹è¯•æ£€ç´¢
            query_results = vector_store.search_similar_chunks_by_query(
                query="ä»€ä¹ˆæ˜¯ç›‘ç£å­¦ä¹ ï¼Ÿ",
                group_id="test_workflow_group",
                top_k=2
            )
            print(f"âœ… æ­¥éª¤4 - æ£€ç´¢æµ‹è¯•: æ‰¾åˆ° {len(query_results)} ä¸ªç›¸å…³ç»“æœ")
            
            # æ¸…ç†æµ‹è¯•æ•°æ®
            vector_store.delete_document_chunks("test_workflow_lit", "test_workflow_group")
            print("âœ… æµ‹è¯•æ•°æ®æ¸…ç†å®Œæˆ")
            
            return True
        else:
            print("âŒ æ— æ³•ç”Ÿæˆembeddings")
            return False
            
    except Exception as e:
        print(f"âŒ ç«¯åˆ°ç«¯æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ æ–‡çŒ®ç®¡ç†ç³»ç»Ÿç»¼åˆåŠŸèƒ½æµ‹è¯•")
    print("=" * 60)
    
    tests = [
        ("Google APIè¿æ¥", test_google_api),
        ("EmbeddingæœåŠ¡", test_embedding_service),
        ("å‘é‡æ•°æ®åº“", test_vector_store),
        ("æ–‡æœ¬å¤„ç†", test_text_processing),
        ("æ–‡ä»¶å¤„ç†", test_file_upload_processing),
        ("ç«¯åˆ°ç«¯å·¥ä½œæµç¨‹", test_end_to_end_workflow)
    ]
    
    results = []
    total_time = time.time()
    
    for test_name, test_func in tests:
        print(f"\n{'='*20} {test_name} {'='*20}")
        start_time = time.time()
        
        try:
            success = test_func()
            results.append((test_name, success))
            status = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
            elapsed = time.time() - start_time
            print(f"\n{status} ({elapsed:.2f}ç§’)")
        except Exception as e:
            results.append((test_name, False))
            elapsed = time.time() - start_time
            print(f"\nâŒ å¼‚å¸¸: {e} ({elapsed:.2f}ç§’)")
    
    # æµ‹è¯•ç»“æœæ±‡æ€»
    total_elapsed = time.time() - total_time
    print(f"\n{'='*60}")
    print("ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»")
    print(f"{'='*60}")
    
    passed = 0
    for test_name, success in results:
        status = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
        print(f"{test_name:<20} {status}")
        if success:
            passed += 1
    
    print(f"\næ€»è®¡: {passed}/{len(results)} é€šè¿‡")
    print(f"æ€»è€—æ—¶: {total_elapsed:.2f}ç§’")
    
    if passed == len(results):
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç³»ç»ŸåŠŸèƒ½å®Œæ•´ã€‚")
    else:
        print(f"\nâš ï¸  {len(results) - passed} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³åŠŸèƒ½ã€‚")
    
    print("\nğŸ’¡ ç³»ç»ŸåŠŸèƒ½æ¦‚è§ˆ:")
    print("   âœ… Google AI APIé›†æˆ")
    print("   âœ… æ–‡æœ¬å‘é‡åŒ–æœåŠ¡")
    print("   âœ… ChromaDBå‘é‡æ•°æ®åº“")
    print("   âœ… æ™ºèƒ½æ–‡æœ¬å¤„ç†")
    print("   âœ… æ–‡ä»¶æå–å’Œè§£æ")
    print("   âœ… å®Œæ•´çš„æ£€ç´¢å·¥ä½œæµç¨‹")

if __name__ == "__main__":
    main() 