#!/usr/bin/env python3
"""
æ–‡æœ¬å¤„ç†åŠŸèƒ½æµ‹è¯•è„šæœ¬
æµ‹è¯•æ–‡æœ¬æå–ã€åˆ†å—ã€å¼‚æ­¥å¤„ç†ç­‰åŠŸèƒ½
"""

import os
import sys
import tempfile
import time
import requests
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.utils.text_extractor import (
    extract_pdf_text, 
    extract_docx_text, 
    extract_html_text,
    clean_extracted_text,
    extract_title_from_text,
    extract_text_from_file
)
from app.utils.text_processor import (
    split_text_into_chunks,
    prepare_chunks_for_embedding,
    estimate_token_count,
    extract_keywords,
    validate_chunk_quality
)
from app.utils.async_processor import AsyncProcessor
from app.config import settings

# APIåŸºç¡€URL
BASE_URL = "http://localhost:8000"

class TextProcessingTester:
    def __init__(self):
        self.token = None
        self.user_id = None
        self.group_id = None
        self.literature_id = None
        self.async_processor = AsyncProcessor()
    
    def login(self):
        """ç™»å½•è·å–token"""
        print("ğŸ” ç”¨æˆ·ç™»å½•...")
        
        login_data = {
            "username": "testuser",
            "password": "testpass123"
        }
        
        response = requests.post(f"{BASE_URL}/login", data=login_data)
        
        if response.status_code == 200:
            result = response.json()
            self.token = result["access_token"]
            import jwt
            payload = jwt.decode(result["access_token"], options={"verify_signature": False})
            self.user_id = payload.get("sub")
            print(f"   âœ… ç™»å½•æˆåŠŸ: {self.user_id}")
            return True
        else:
            print(f"   âŒ ç™»å½•å¤±è´¥: {response.text}")
            return False
    
    def get_headers(self):
        """è·å–è®¤è¯å¤´"""
        return {"Authorization": f"Bearer {self.token}"}
    
    def get_user_groups(self):
        """è·å–ç”¨æˆ·ç ”ç©¶ç»„"""
        print("\nğŸ“‹ è·å–ç”¨æˆ·ç ”ç©¶ç»„...")
        
        response = requests.get(f"{BASE_URL}/user/groups", headers=self.get_headers())
        
        if response.status_code == 200:
            groups = response.json()["groups"]
            if groups:
                self.group_id = groups[0]["id"]
                print(f"   âœ… æ‰¾åˆ°ç ”ç©¶ç»„: {groups[0]['name']} ({self.group_id})")
                return True
            else:
                print("   âŒ ç”¨æˆ·æ²¡æœ‰åŠ å…¥ä»»ä½•ç ”ç©¶ç»„")
                return False
        else:
            print(f"   âŒ è·å–ç ”ç©¶ç»„å¤±è´¥: {response.text}")
            return False
    
    def test_text_extraction(self):
        """æµ‹è¯•æ–‡æœ¬æå–åŠŸèƒ½"""
        print("\nğŸ“„ æµ‹è¯•æ–‡æœ¬æå–åŠŸèƒ½...")
        
        # æµ‹è¯•PDFæ–‡æœ¬æå–
        print("   æµ‹è¯•PDFæ–‡æœ¬æå–...")
        # ç®€å•çš„PDFå†…å®¹ç”¨äºæµ‹è¯•
        pdf_content = b"%PDF-1.4\n1 0 obj\n<<\n/Type /Catalog\n/Pages 2 0 R\n>>\nendobj\n2 0 obj\n<<\n/Type /Pages\n/Kids [3 0 R]\n/Count 1\n>>\nendobj\n3 0 obj\n<<\n/Type /Page\n/Parent 2 0 R\n/MediaBox [0 0 612 792]\n>>\nendobj\nxref\n0 4\n0000000000 65535 f \n0000000009 00000 n \n0000000074 00000 n \n0000000120 00000 n \ntrailer\n<<\n/Size 4\n/Root 1 0 R\n>>\nstartxref\n179\n%%EOF"
        
        with tempfile.NamedTemporaryFile(suffix=".pdf", delete=False) as tmp_pdf:
            tmp_pdf.write(pdf_content)
            tmp_pdf_path = tmp_pdf.name
        
        try:
            pdf_text = extract_pdf_text(tmp_pdf_path)
            if pdf_text is not None:
                print(f"      âœ… PDFæ–‡æœ¬æå–æˆåŠŸ: {len(pdf_text)} å­—ç¬¦")
            else:
                print("      âš ï¸  PDFæ–‡æœ¬æå–è¿”å›ç©º")
        except Exception as e:
            print(f"      âŒ PDFæ–‡æœ¬æå–å¤±è´¥: {e}")
        finally:
            if os.path.exists(tmp_pdf_path):
                os.unlink(tmp_pdf_path)
        
        # æµ‹è¯•HTMLæ–‡æœ¬æå–
        print("   æµ‹è¯•HTMLæ–‡æœ¬æå–...")
        html_content = """
        <!DOCTYPE html>
        <html>
        <head>
            <title>æµ‹è¯•æ–‡æ¡£</title>
            <style>body { font-family: Arial; }</style>
        </head>
        <body>
            <h1>è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ ‡é¢˜</h1>
            <p>è¿™æ˜¯ç¬¬ä¸€æ®µå†…å®¹ï¼ŒåŒ…å«ä¸€äº›<strong>é‡è¦ä¿¡æ¯</strong>ã€‚</p>
            <p>è¿™æ˜¯ç¬¬äºŒæ®µå†…å®¹ï¼Œè®¨è®ºäº†æ–‡æœ¬å¤„ç†çš„é‡è¦æ€§ã€‚</p>
            <script>console.log('è¿™æ®µè„šæœ¬åº”è¯¥è¢«å¿½ç•¥');</script>
        </body>
        </html>
        """
        
        with tempfile.NamedTemporaryFile(mode='w', suffix=".html", delete=False, encoding='utf-8') as tmp_html:
            tmp_html.write(html_content)
            tmp_html_path = tmp_html.name
        
        try:
            html_text = extract_html_text(tmp_html_path)
            if html_text:
                print(f"      âœ… HTMLæ–‡æœ¬æå–æˆåŠŸ: {len(html_text)} å­—ç¬¦")
                print(f"      å†…å®¹é¢„è§ˆ: {html_text[:100]}...")
            else:
                print("      âš ï¸  HTMLæ–‡æœ¬æå–è¿”å›ç©º")
        except Exception as e:
            print(f"      âŒ HTMLæ–‡æœ¬æå–å¤±è´¥: {e}")
        finally:
            if os.path.exists(tmp_html_path):
                os.unlink(tmp_html_path)
        
        # æµ‹è¯•æ–‡æœ¬æ¸…ç†
        print("   æµ‹è¯•æ–‡æœ¬æ¸…ç†...")
        dirty_text = "   è¿™æ˜¯ä¸€ä¸ª\n\n\nåŒ…å«å¤šä½™ç©ºç™½\t\tçš„æ–‡æœ¬   \n\r\n  "
        cleaned_text = clean_extracted_text(dirty_text)
        print(f"      åŸæ–‡æœ¬: '{dirty_text}'")
        print(f"      æ¸…ç†å: '{cleaned_text}'")
        
        # æµ‹è¯•æ ‡é¢˜æå–
        print("   æµ‹è¯•æ ‡é¢˜æå–...")
        sample_text = """
        
        æ·±åº¦å­¦ä¹ åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„åº”ç”¨ç ”ç©¶
        
        æ‘˜è¦ï¼šæœ¬æ–‡æ¢è®¨äº†æ·±åº¦å­¦ä¹ æŠ€æœ¯åœ¨è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸçš„æœ€æ–°è¿›å±•...
        
        1. å¼•è¨€
        è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦åˆ†æ”¯...
        """
        title = extract_title_from_text(sample_text)
        print(f"      æå–çš„æ ‡é¢˜: '{title}'")
        
        return True
    
    def test_text_chunking(self):
        """æµ‹è¯•æ–‡æœ¬åˆ†å—åŠŸèƒ½"""
        print("\nğŸ”ª æµ‹è¯•æ–‡æœ¬åˆ†å—åŠŸèƒ½...")
        
        # å‡†å¤‡æµ‹è¯•æ–‡æœ¬
        test_text = """
        äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligenceï¼ŒAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä¼å›¾äº†è§£æ™ºèƒ½çš„å®è´¨ï¼Œå¹¶ç”Ÿäº§å‡ºä¸€ç§æ–°çš„èƒ½ä»¥äººç±»æ™ºèƒ½ç›¸ä¼¼çš„æ–¹å¼åšå‡ºååº”çš„æ™ºèƒ½æœºå™¨ã€‚

        è¯¥é¢†åŸŸçš„ç ”ç©¶åŒ…æ‹¬æœºå™¨äººã€è¯­è¨€è¯†åˆ«ã€å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œä¸“å®¶ç³»ç»Ÿç­‰ã€‚äººå·¥æ™ºèƒ½ä»è¯ç”Ÿä»¥æ¥ï¼Œç†è®ºå’ŒæŠ€æœ¯æ—¥ç›Šæˆç†Ÿï¼Œåº”ç”¨é¢†åŸŸä¹Ÿä¸æ–­æ‰©å¤§ã€‚

        æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œå®ƒæ˜¯ä¸€ç§é€šè¿‡ç®—æ³•ä½¿æœºå™¨èƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ å¹¶åšå‡ºå†³ç­–æˆ–é¢„æµ‹çš„æŠ€æœ¯ã€‚æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é›†ï¼Œå®ƒä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å·¥ä½œæ–¹å¼ã€‚

        è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNatural Language Processingï¼ŒNLPï¼‰æ˜¯äººå·¥æ™ºèƒ½å’Œè¯­è¨€å­¦é¢†åŸŸçš„åˆ†æ”¯å­¦ç§‘ã€‚æ­¤é¢†åŸŸæ¢è®¨å¦‚ä½•å¤„ç†åŠè¿ç”¨è‡ªç„¶è¯­è¨€ã€‚

        è®¡ç®—æœºè§†è§‰æ˜¯ä¸€é—¨ç ”ç©¶å¦‚ä½•ä½¿æœºå™¨"çœ‹"çš„ç§‘å­¦ï¼Œæ›´è¿›ä¸€æ­¥çš„è¯´ï¼Œå°±æ˜¯æ˜¯æŒ‡ç”¨æ‘„å½±æœºå’Œç”µè„‘ä»£æ›¿äººçœ¼å¯¹ç›®æ ‡è¿›è¡Œè¯†åˆ«ã€è·Ÿè¸ªå’Œæµ‹é‡ç­‰æœºå™¨è§†è§‰ï¼Œå¹¶è¿›ä¸€æ­¥åšå›¾å½¢å¤„ç†ã€‚
        """
        
        # æµ‹è¯•åŸºæœ¬åˆ†å—
        print(f"   åŸæ–‡æœ¬é•¿åº¦: {len(test_text)} å­—ç¬¦")
        print(f"   é…ç½®çš„åˆ†å—å¤§å°: {settings.CHUNK_SIZE}")
        print(f"   é…ç½®çš„é‡å å¤§å°: {settings.CHUNK_OVERLAP}")
        
        chunks = split_text_into_chunks(test_text)
        print(f"   âœ… åˆ†å—å®Œæˆ: ç”Ÿæˆ {len(chunks)} ä¸ªæ–‡æœ¬å—")
        
        for i, chunk in enumerate(chunks):
            print(f"      å— {i+1}: {len(chunk)} å­—ç¬¦")
            print(f"         é¢„è§ˆ: {chunk[:50]}...")
        
        # æµ‹è¯•è‡ªå®šä¹‰å‚æ•°åˆ†å—
        print("\n   æµ‹è¯•è‡ªå®šä¹‰å‚æ•°åˆ†å—...")
        custom_chunks = split_text_into_chunks(test_text, chunk_size=200, overlap=50)
        print(f"   âœ… è‡ªå®šä¹‰åˆ†å—å®Œæˆ: ç”Ÿæˆ {len(custom_chunks)} ä¸ªæ–‡æœ¬å—")
        
        # æµ‹è¯•å‡†å¤‡embeddingæ•°æ®
        print("\n   æµ‹è¯•å‡†å¤‡embeddingæ•°æ®...")
        chunks_data = prepare_chunks_for_embedding(
            chunks, 
            "test_lit_123", 
            "test_group_456",
            "äººå·¥æ™ºèƒ½æŠ€æœ¯æ¦‚è¿°"
        )
        print(f"   âœ… æ•°æ®å‡†å¤‡å®Œæˆ: {len(chunks_data)} ä¸ªæ•°æ®å—")
        
        if chunks_data:
            sample_data = chunks_data[0]
            print(f"      æ ·æœ¬æ•°æ®ç»“æ„: {list(sample_data.keys())}")
            print(f"      æ–‡æœ¬é•¿åº¦: {sample_data['chunk_length']}")
            print(f"      å—ID: {sample_data['chunk_id']}")
        
        return True
    
    def test_token_estimation(self):
        """æµ‹è¯•tokenè®¡ç®—åŠŸèƒ½"""
        print("\nğŸ”¢ æµ‹è¯•tokenè®¡ç®—åŠŸèƒ½...")
        
        test_texts = [
            "Hello world!",
            "ä½ å¥½ï¼Œä¸–ç•Œï¼",
            "This is a longer text with multiple sentences. It contains both English and Chinese characters. è¿™æ®µæ–‡æœ¬åŒ…å«äº†è‹±æ–‡å’Œä¸­æ–‡å­—ç¬¦ã€‚",
            "A" * 1000  # é•¿æ–‡æœ¬æµ‹è¯•
        ]
        
        for i, text in enumerate(test_texts):
            print(f"   æ–‡æœ¬ {i+1}: {len(text)} å­—ç¬¦")
            
            # æµ‹è¯•OpenAI tokenè®¡ç®—
            try:
                openai_tokens = estimate_token_count(text, "openai")
                print(f"      OpenAIä¼°ç®—: {openai_tokens} tokens")
            except Exception as e:
                print(f"      OpenAIä¼°ç®—å¤±è´¥: {e}")
            
            # æµ‹è¯•Google tokenè®¡ç®—
            try:
                google_tokens = estimate_token_count(text, "google")
                print(f"      Googleä¼°ç®—: {google_tokens} tokens")
            except Exception as e:
                print(f"      Googleä¼°ç®—å¤±è´¥: {e}")
        
        return True
    
    def test_keyword_extraction(self):
        """æµ‹è¯•å…³é”®è¯æå–åŠŸèƒ½"""
        print("\nğŸ” æµ‹è¯•å…³é”®è¯æå–åŠŸèƒ½...")
        
        test_text = """
        æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒåŸºäºäººå·¥ç¥ç»ç½‘ç»œçš„ç ”ç©¶ï¼Œç‰¹åˆ«æ˜¯åˆ©ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥è¿›è¡Œå­¦ä¹ å’Œæ¨¡å¼è¯†åˆ«ã€‚
        æ·±åº¦å­¦ä¹ æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ æ•°æ®çš„é«˜å±‚æ¬¡ç‰¹å¾ï¼Œè¿™äº›ç‰¹å¾å¯¹äºå›¾åƒè¯†åˆ«ã€è¯­éŸ³è¯†åˆ«å’Œè‡ªç„¶è¯­è¨€å¤„ç†ç­‰ä»»åŠ¡éå¸¸æœ‰ç”¨ã€‚
        å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰åœ¨å›¾åƒå¤„ç†é¢†åŸŸè¡¨ç°å‡ºè‰²ï¼Œè€Œå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰å’Œé•¿çŸ­æœŸè®°å¿†ç½‘ç»œï¼ˆLSTMï¼‰åœ¨åºåˆ—æ•°æ®å¤„ç†æ–¹é¢å¾ˆæœ‰æ•ˆã€‚
        """
        
        try:
            keywords = extract_keywords(test_text, max_keywords=8)
            print(f"   âœ… å…³é”®è¯æå–æˆåŠŸ: {len(keywords)} ä¸ªå…³é”®è¯")
            print(f"      å…³é”®è¯: {', '.join(keywords)}")
        except Exception as e:
            print(f"   âŒ å…³é”®è¯æå–å¤±è´¥: {e}")
        
        return True
    
    def test_chunk_quality_validation(self):
        """æµ‹è¯•æ–‡æœ¬å—è´¨é‡éªŒè¯"""
        print("\nâœ… æµ‹è¯•æ–‡æœ¬å—è´¨é‡éªŒè¯...")
        
        test_chunks = [
            "è¿™æ˜¯ä¸€ä¸ªé«˜è´¨é‡çš„æ–‡æœ¬å—ï¼ŒåŒ…å«å®Œæ•´çš„å¥å­å’Œæœ‰æ„ä¹‰çš„å†…å®¹ã€‚å®ƒæœ‰è¶³å¤Ÿçš„é•¿åº¦å’Œæ¸…æ™°çš„è¡¨è¾¾ã€‚",
            "çŸ­æ–‡æœ¬",
            "A" * 2000,  # è¿‡é•¿æ–‡æœ¬
            "123456789",  # çº¯æ•°å­—
            "è¿™æ˜¯ä¸€ä¸ªåŒ…å«ç‰¹æ®Šå­—ç¬¦çš„æ–‡æœ¬å—ï¼@#$%^&*()ï¼Œä½†ä»ç„¶æœ‰æ„ä¹‰ã€‚"
        ]
        
        for i, chunk in enumerate(test_chunks):
            print(f"   æ–‡æœ¬å— {i+1}: {len(chunk)} å­—ç¬¦")
            try:
                quality = validate_chunk_quality(chunk)
                print(f"      è´¨é‡è¯„åˆ†: {quality['score']:.2f}")
                print(f"      æ˜¯å¦æœ‰æ•ˆ: {quality['is_valid']}")
                if quality['issues']:
                    print(f"      é—®é¢˜: {', '.join(quality['issues'])}")
            except Exception as e:
                print(f"      è´¨é‡éªŒè¯å¤±è´¥: {e}")
        
        return True
    
    def test_async_processing(self):
        """æµ‹è¯•å¼‚æ­¥å¤„ç†åŠŸèƒ½"""
        print("\nâš¡ æµ‹è¯•å¼‚æ­¥å¤„ç†åŠŸèƒ½...")
        
        # é¦–å…ˆä¸Šä¼ ä¸€ä¸ªæµ‹è¯•æ–‡ä»¶
        print("   ä¸Šä¼ æµ‹è¯•æ–‡ä»¶...")
        # ç®€å•çš„PDFå†…å®¹ç”¨äºæµ‹è¯•
        test_content = b"%PDF-1.4\n1 0 obj\n<<\n/Type /Catalog\n/Pages 2 0 R\n>>\nendobj\n2 0 obj\n<<\n/Type /Pages\n/Kids [3 0 R]\n/Count 1\n>>\nendobj\n3 0 obj\n<<\n/Type /Page\n/Parent 2 0 R\n/MediaBox [0 0 612 792]\n>>\nendobj\nxref\n0 4\n0000000000 65535 f \n0000000009 00000 n \n0000000074 00000 n \n0000000120 00000 n \ntrailer\n<<\n/Size 4\n/Root 1 0 R\n>>\nstartxref\n179\n%%EOF"
        
        with tempfile.NamedTemporaryFile(suffix=".pdf", delete=False) as tmp_file:
            tmp_file.write(test_content)
            tmp_file_path = tmp_file.name
        
        try:
            with open(tmp_file_path, "rb") as f:
                files = {"file": ("async_test.pdf", f, "application/pdf")}
                data = {
                    "group_id": self.group_id,
                    "title": "å¼‚æ­¥å¤„ç†æµ‹è¯•æ–‡æ¡£"
                }
                
                response = requests.post(
                    f"{BASE_URL}/literature/upload",
                    files=files,
                    data=data,
                    headers=self.get_headers()
                )
            
            if response.status_code == 200:
                result = response.json()
                literature_id = result["literature_id"]
                print(f"      âœ… æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {literature_id}")
                
                # æµ‹è¯•å¼‚æ­¥å¤„ç†
                print("   å¯åŠ¨å¼‚æ­¥å¤„ç†...")
                task_id = self.async_processor.process_literature_async(literature_id)
                print(f"      ä»»åŠ¡ID: {task_id}")
                
                # ç›‘æ§å¤„ç†è¿›åº¦
                print("   ç›‘æ§å¤„ç†è¿›åº¦...")
                for i in range(10):  # æœ€å¤šç­‰å¾…10ç§’
                    time.sleep(1)
                    status = self.async_processor.get_task_status(task_id)
                    if status:
                        print(f"      è¿›åº¦: {status['progress']}% - {status['message']}")
                        if status['status'] in ['completed', 'failed']:
                            break
                
                # è·å–æœ€ç»ˆçŠ¶æ€
                final_status = self.async_processor.get_task_status(task_id)
                if final_status:
                    if final_status['status'] == 'completed':
                        print(f"      âœ… å¼‚æ­¥å¤„ç†æˆåŠŸå®Œæˆ")
                        if 'data' in final_status:
                            data = final_status['data']
                            print(f"         æ–‡æœ¬å—æ•°: {data.get('chunks_count', 0)}")
                            print(f"         æ–‡æœ¬é•¿åº¦: {data.get('text_length', 0)}")
                    else:
                        print(f"      âŒ å¼‚æ­¥å¤„ç†å¤±è´¥: {final_status['message']}")
                else:
                    print("      âš ï¸  æ— æ³•è·å–å¤„ç†çŠ¶æ€")
                
                return True
            else:
                print(f"      âŒ æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {response.text}")
                return False
                
        finally:
            if os.path.exists(tmp_file_path):
                try:
                    os.unlink(tmp_file_path)
                except:
                    pass
    
    def test_configuration(self):
        """æµ‹è¯•é…ç½®ä¿¡æ¯"""
        print("\nâš™ï¸  æµ‹è¯•é…ç½®ä¿¡æ¯...")
        
        print(f"   åˆ†å—å¤§å°: {settings.CHUNK_SIZE}")
        print(f"   åˆ†å—é‡å : {settings.CHUNK_OVERLAP}")
        print(f"   æœ€å¤§æ£€ç´¢æ–‡æ¡£æ•°: {settings.MAX_RETRIEVAL_DOCS}")
        print(f"   AIæä¾›å•†: {settings.get_ai_provider()}")
        
        ai_valid, ai_message = settings.validate_ai_config()
        print(f"   AIé…ç½®: {'âœ…' if ai_valid else 'âŒ'} {ai_message}")
        
        return True
    
    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("ğŸ§ª æ–‡æœ¬å¤„ç†åŠŸèƒ½æµ‹è¯•")
        print("="*50)
        
        tests = [
            ("é…ç½®ä¿¡æ¯", self.test_configuration),
            ("æ–‡æœ¬æå–", self.test_text_extraction),
            ("æ–‡æœ¬åˆ†å—", self.test_text_chunking),
            ("Tokenè®¡ç®—", self.test_token_estimation),
            ("å…³é”®è¯æå–", self.test_keyword_extraction),
            ("æ–‡æœ¬å—è´¨é‡éªŒè¯", self.test_chunk_quality_validation),
            ("ç™»å½•", self.login),
            ("è·å–ç ”ç©¶ç»„", self.get_user_groups),
            ("å¼‚æ­¥å¤„ç†", self.test_async_processing)
        ]
        
        passed = 0
        total = len(tests)
        
        for test_name, test_func in tests:
            try:
                print(f"\n{'='*20} {test_name} {'='*20}")
                if test_func():
                    passed += 1
                    print(f"âœ… {test_name} æµ‹è¯•é€šè¿‡")
                else:
                    print(f"âŒ {test_name} æµ‹è¯•å¤±è´¥")
            except Exception as e:
                print(f"âŒ {test_name} æµ‹è¯•å¼‚å¸¸: {e}")
        
        print(f"\nğŸ“Š æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
        
        if passed == total:
            print("ğŸ‰ æ‰€æœ‰æ–‡æœ¬å¤„ç†åŠŸèƒ½æµ‹è¯•é€šè¿‡!")
        else:
            print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®ç°")

def main():
    """ä¸»å‡½æ•°"""
    tester = TextProcessingTester()
    tester.run_all_tests()

if __name__ == "__main__":
    main() 