#!/usr/bin/env python3
"""
æ ¸å¿ƒæ–‡æœ¬å¤„ç†åŠŸèƒ½æµ‹è¯•è„šæœ¬
ä¸“é—¨æµ‹è¯•æ–‡æœ¬æå–ã€åˆ†å—ã€tokenè®¡ç®—ç­‰æ ¸å¿ƒåŠŸèƒ½
"""

import os
import sys
import tempfile

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.utils.text_extractor import (
    clean_extracted_text,
    extract_title_from_text,
    extract_html_text
)
from app.utils.text_processor import (
    split_text_into_chunks,
    prepare_chunks_for_embedding,
    estimate_token_count,
    extract_keywords,
    validate_chunk_quality
)
from app.config import settings

def test_text_cleaning():
    """æµ‹è¯•æ–‡æœ¬æ¸…ç†åŠŸèƒ½"""
    print("ğŸ§¹ æµ‹è¯•æ–‡æœ¬æ¸…ç†åŠŸèƒ½...")
    
    test_cases = [
        ("   å¤šä½™ç©ºç™½   \n\n\n  ", "å¤šä½™ç©ºç™½"),
        ("æ­£å¸¸æ–‡æœ¬", "æ­£å¸¸æ–‡æœ¬"),
        ("åŒ…å«\tåˆ¶è¡¨ç¬¦\r\nçš„æ–‡æœ¬", "åŒ…å« åˆ¶è¡¨ç¬¦ çš„æ–‡æœ¬"),
        ("", ""),
    ]
    
    for dirty, expected in test_cases:
        cleaned = clean_extracted_text(dirty)
        status = "âœ…" if cleaned.strip() == expected.strip() else "âŒ"
        print(f"   {status} '{dirty}' -> '{cleaned}'")
    
    return True

def test_title_extraction():
    """æµ‹è¯•æ ‡é¢˜æå–åŠŸèƒ½"""
    print("\nğŸ“ æµ‹è¯•æ ‡é¢˜æå–åŠŸèƒ½...")
    
    test_texts = [
        "æ·±åº¦å­¦ä¹ åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„åº”ç”¨\n\næ‘˜è¦ï¼šæœ¬æ–‡...",
        "ç¬¬ä¸€ç«  å¼•è¨€\n\näººå·¥æ™ºèƒ½æ˜¯...",
        "Abstract\n\nThis paper discusses...",
        "å¾ˆçŸ­",
        ""
    ]
    
    for text in test_texts:
        title = extract_title_from_text(text)
        print(f"   æ–‡æœ¬: '{text[:30]}...' -> æ ‡é¢˜: '{title}'")
    
    return True

def test_html_extraction():
    """æµ‹è¯•HTMLæ–‡æœ¬æå–"""
    print("\nğŸŒ æµ‹è¯•HTMLæ–‡æœ¬æå–...")
    
    html_content = """
    <html>
    <head><title>æµ‹è¯•</title></head>
    <body>
        <h1>ä¸»æ ‡é¢˜</h1>
        <p>è¿™æ˜¯ç¬¬ä¸€æ®µã€‚</p>
        <p>è¿™æ˜¯ç¬¬äºŒæ®µã€‚</p>
        <script>alert('è¿™åº”è¯¥è¢«å¿½ç•¥');</script>
        <style>body { color: red; }</style>
    </body>
    </html>
    """
    
    with tempfile.NamedTemporaryFile(mode='w', suffix=".html", delete=False, encoding='utf-8') as tmp_file:
        tmp_file.write(html_content)
        tmp_file_path = tmp_file.name
    
    try:
        extracted_text = extract_html_text(tmp_file_path)
        print(f"   æå–çš„æ–‡æœ¬: '{extracted_text}'")
        print(f"   æ–‡æœ¬é•¿åº¦: {len(extracted_text)} å­—ç¬¦")
        
        # éªŒè¯è„šæœ¬å’Œæ ·å¼è¢«ç§»é™¤
        if "alert" not in extracted_text and "color: red" not in extracted_text:
            print("   âœ… è„šæœ¬å’Œæ ·å¼å·²æ­£ç¡®ç§»é™¤")
        else:
            print("   âŒ è„šæœ¬æˆ–æ ·å¼æœªè¢«ç§»é™¤")
        
        return True
    except Exception as e:
        print(f"   âŒ HTMLæå–å¤±è´¥: {e}")
        return False
    finally:
        if os.path.exists(tmp_file_path):
            os.unlink(tmp_file_path)

def test_text_chunking():
    """æµ‹è¯•æ–‡æœ¬åˆ†å—åŠŸèƒ½"""
    print("\nğŸ”ª æµ‹è¯•æ–‡æœ¬åˆ†å—åŠŸèƒ½...")
    
    # åˆ›å»ºä¸€ä¸ªè¾ƒé•¿çš„æµ‹è¯•æ–‡æœ¬
    long_text = """
    äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligenceï¼ŒAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä¼å›¾äº†è§£æ™ºèƒ½çš„å®è´¨ï¼Œå¹¶ç”Ÿäº§å‡ºä¸€ç§æ–°çš„èƒ½ä»¥äººç±»æ™ºèƒ½ç›¸ä¼¼çš„æ–¹å¼åšå‡ºååº”çš„æ™ºèƒ½æœºå™¨ã€‚è¯¥é¢†åŸŸçš„ç ”ç©¶åŒ…æ‹¬æœºå™¨äººã€è¯­è¨€è¯†åˆ«ã€å›¾åƒè¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œä¸“å®¶ç³»ç»Ÿç­‰ã€‚

    äººå·¥æ™ºèƒ½ä»è¯ç”Ÿä»¥æ¥ï¼Œç†è®ºå’ŒæŠ€æœ¯æ—¥ç›Šæˆç†Ÿï¼Œåº”ç”¨é¢†åŸŸä¹Ÿä¸æ–­æ‰©å¤§ï¼Œå¯ä»¥è®¾æƒ³ï¼Œæœªæ¥äººå·¥æ™ºèƒ½å¸¦æ¥çš„ç§‘æŠ€äº§å“ï¼Œå°†ä¼šæ˜¯äººç±»æ™ºæ…§çš„"å®¹å™¨"ã€‚äººå·¥æ™ºèƒ½å¯ä»¥å¯¹äººçš„æ„è¯†ã€æ€ç»´çš„ä¿¡æ¯è¿‡ç¨‹çš„æ¨¡æ‹Ÿã€‚

    æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œå®ƒæ˜¯ä¸€ç§é€šè¿‡ç®—æ³•ä½¿æœºå™¨èƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ å¹¶åšå‡ºå†³ç­–æˆ–é¢„æµ‹çš„æŠ€æœ¯ã€‚æœºå™¨å­¦ä¹ çš„æ ¸å¿ƒæ€æƒ³æ˜¯è®©è®¡ç®—æœºé€šè¿‡å¤§é‡æ•°æ®çš„è®­ç»ƒï¼Œè‡ªåŠ¨å‘ç°æ•°æ®ä¸­çš„æ¨¡å¼å’Œè§„å¾‹ã€‚

    æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é›†ï¼Œå®ƒä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å·¥ä½œæ–¹å¼ã€‚æ·±åº¦å­¦ä¹ åœ¨å›¾åƒè¯†åˆ«ã€è¯­éŸ³è¯†åˆ«ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚

    è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNatural Language Processingï¼ŒNLPï¼‰æ˜¯äººå·¥æ™ºèƒ½å’Œè¯­è¨€å­¦é¢†åŸŸçš„åˆ†æ”¯å­¦ç§‘ã€‚æ­¤é¢†åŸŸæ¢è®¨å¦‚ä½•å¤„ç†åŠè¿ç”¨è‡ªç„¶è¯­è¨€ï¼›è‡ªç„¶è¯­è¨€å¤„ç†åŒ…æ‹¬å¤šä¸ªæ–¹é¢å’Œæ­¥éª¤ï¼ŒåŸºæœ¬æœ‰è®¤çŸ¥ã€ç†è§£ã€ç”Ÿæˆç­‰éƒ¨åˆ†ã€‚
    """ * 3  # é‡å¤3æ¬¡ä»¥ç¡®ä¿æ–‡æœ¬è¶³å¤Ÿé•¿
    
    print(f"   åŸå§‹æ–‡æœ¬é•¿åº¦: {len(long_text)} å­—ç¬¦")
    print(f"   é…ç½®çš„åˆ†å—å¤§å°: {settings.CHUNK_SIZE}")
    print(f"   é…ç½®çš„é‡å å¤§å°: {settings.CHUNK_OVERLAP}")
    
    # æµ‹è¯•é»˜è®¤åˆ†å—
    chunks = split_text_into_chunks(long_text)
    print(f"   âœ… é»˜è®¤åˆ†å—: ç”Ÿæˆ {len(chunks)} ä¸ªå—")
    
    for i, chunk in enumerate(chunks[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
        print(f"      å— {i+1}: {len(chunk)} å­—ç¬¦, é¢„è§ˆ: {chunk[:50]}...")
    
    # æµ‹è¯•è‡ªå®šä¹‰åˆ†å—
    custom_chunks = split_text_into_chunks(long_text, chunk_size=500, overlap=100)
    print(f"   âœ… è‡ªå®šä¹‰åˆ†å—(500/100): ç”Ÿæˆ {len(custom_chunks)} ä¸ªå—")
    
    # æµ‹è¯•å‡†å¤‡embeddingæ•°æ®
    chunks_data = prepare_chunks_for_embedding(
        chunks[:2],  # åªç”¨å‰2ä¸ªå—
        "test_literature_123",
        "test_group_456",
        "äººå·¥æ™ºèƒ½æŠ€æœ¯æ¦‚è¿°"
    )
    
    print(f"   âœ… Embeddingæ•°æ®å‡†å¤‡: {len(chunks_data)} ä¸ªæ•°æ®å—")
    if chunks_data:
        sample = chunks_data[0]
        print(f"      æ ·æœ¬ç»“æ„: {list(sample.keys())}")
        print(f"      å—ID: {sample['chunk_id']}")
    
    return True

def test_token_estimation():
    """æµ‹è¯•tokenè®¡ç®—"""
    print("\nğŸ”¢ æµ‹è¯•tokenè®¡ç®—åŠŸèƒ½...")
    
    test_texts = [
        "Hello world!",
        "ä½ å¥½ï¼Œä¸–ç•Œï¼",
        "This is a test sentence with both English and Chinese è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•å¥å­ã€‚",
        "A" * 100,  # é‡å¤å­—ç¬¦
        "The quick brown fox jumps over the lazy dog. " * 10  # é‡å¤å¥å­
    ]
    
    for i, text in enumerate(test_texts):
        print(f"   æ–‡æœ¬ {i+1} ({len(text)} å­—ç¬¦):")
        
        try:
            openai_tokens = estimate_token_count(text, "openai")
            print(f"      OpenAIä¼°ç®—: {openai_tokens} tokens")
        except Exception as e:
            print(f"      OpenAIä¼°ç®—å¤±è´¥: {e}")
        
        try:
            google_tokens = estimate_token_count(text, "google")
            print(f"      Googleä¼°ç®—: {google_tokens} tokens")
        except Exception as e:
            print(f"      Googleä¼°ç®—å¤±è´¥: {e}")
    
    return True

def test_keyword_extraction():
    """æµ‹è¯•å…³é”®è¯æå–"""
    print("\nğŸ” æµ‹è¯•å…³é”®è¯æå–åŠŸèƒ½...")
    
    test_text = """
    æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œå®ƒåŸºäºäººå·¥ç¥ç»ç½‘ç»œçš„ç ”ç©¶ã€‚
    æ·±åº¦å­¦ä¹ æ¨¡å‹èƒ½å¤Ÿè‡ªåŠ¨å­¦ä¹ æ•°æ®çš„ç‰¹å¾è¡¨ç¤ºï¼Œåœ¨å›¾åƒè¯†åˆ«ã€è¯­éŸ³è¯†åˆ«ã€
    è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸå–å¾—äº†æ˜¾è‘—çš„æˆæœã€‚å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰åœ¨
    è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ï¼Œå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰å’Œé•¿çŸ­æœŸè®°å¿†
    ç½‘ç»œï¼ˆLSTMï¼‰åœ¨åºåˆ—æ•°æ®å¤„ç†æ–¹é¢å¾ˆæœ‰æ•ˆæœã€‚
    """
    
    try:
        keywords = extract_keywords(test_text, max_keywords=10)
        print(f"   âœ… æå–äº† {len(keywords)} ä¸ªå…³é”®è¯:")
        for i, keyword in enumerate(keywords):
            print(f"      {i+1}. {keyword}")
        return True
    except Exception as e:
        print(f"   âŒ å…³é”®è¯æå–å¤±è´¥: {e}")
        return False

def test_chunk_quality():
    """æµ‹è¯•æ–‡æœ¬å—è´¨é‡éªŒè¯"""
    print("\nâœ… æµ‹è¯•æ–‡æœ¬å—è´¨é‡éªŒè¯...")
    
    test_chunks = [
        ("è¿™æ˜¯ä¸€ä¸ªé«˜è´¨é‡çš„æ–‡æœ¬å—ï¼ŒåŒ…å«å®Œæ•´çš„å¥å­å’Œæœ‰æ„ä¹‰çš„å†…å®¹ã€‚å®ƒæœ‰è¶³å¤Ÿçš„é•¿åº¦å’Œæ¸…æ™°çš„è¡¨è¾¾ï¼Œèƒ½å¤Ÿä¸ºè¯»è€…æä¾›æœ‰ä»·å€¼çš„ä¿¡æ¯ã€‚", "é«˜è´¨é‡æ–‡æœ¬"),
        ("çŸ­", "è¿‡çŸ­æ–‡æœ¬"),
        ("A" * 1000, "é‡å¤å­—ç¬¦"),
        ("1234567890" * 10, "çº¯æ•°å­—"),
        ("è¿™æ˜¯ä¸€ä¸ªä¸­ç­‰è´¨é‡çš„æ–‡æœ¬å—ï¼Œè™½ç„¶ä¸æ˜¯å¾ˆé•¿ï¼Œä½†åŒ…å«äº†ä¸€äº›æœ‰æ„ä¹‰çš„å†…å®¹ã€‚", "ä¸­ç­‰è´¨é‡æ–‡æœ¬"),
        ("", "ç©ºæ–‡æœ¬")
    ]
    
    for chunk, description in test_chunks:
        print(f"   æµ‹è¯•: {description} ({len(chunk)} å­—ç¬¦)")
        try:
            quality = validate_chunk_quality(chunk)
            print(f"      è´¨é‡è¯„åˆ†: {quality['score']:.2f}")
            print(f"      æ˜¯å¦æœ‰æ•ˆ: {quality['is_valid']}")
            if quality['issues']:
                print(f"      é—®é¢˜: {', '.join(quality['issues'])}")
        except Exception as e:
            print(f"      âŒ è´¨é‡éªŒè¯å¤±è´¥: {e}")
    
    return True

def test_configuration():
    """æµ‹è¯•é…ç½®ä¿¡æ¯"""
    print("\nâš™ï¸  æµ‹è¯•é…ç½®ä¿¡æ¯...")
    
    print(f"   åˆ†å—å¤§å°: {settings.CHUNK_SIZE}")
    print(f"   åˆ†å—é‡å : {settings.CHUNK_OVERLAP}")
    print(f"   æœ€å¤§æ£€ç´¢æ–‡æ¡£æ•°: {settings.MAX_RETRIEVAL_DOCS}")
    print(f"   AIæä¾›å•†: {settings.get_ai_provider()}")
    
    ai_valid, ai_message = settings.validate_ai_config()
    print(f"   AIé…ç½®: {'âœ…' if ai_valid else 'âŒ'} {ai_message}")
    
    return True

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ§ª æ ¸å¿ƒæ–‡æœ¬å¤„ç†åŠŸèƒ½æµ‹è¯•")
    print("="*50)
    
    tests = [
        ("é…ç½®ä¿¡æ¯", test_configuration),
        ("æ–‡æœ¬æ¸…ç†", test_text_cleaning),
        ("æ ‡é¢˜æå–", test_title_extraction),
        ("HTMLæå–", test_html_extraction),
        ("æ–‡æœ¬åˆ†å—", test_text_chunking),
        ("Tokenè®¡ç®—", test_token_estimation),
        ("å…³é”®è¯æå–", test_keyword_extraction),
        ("æ–‡æœ¬å—è´¨é‡éªŒè¯", test_chunk_quality)
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        try:
            print(f"\n{'='*20} {test_name} {'='*20}")
            if test_func():
                passed += 1
                print(f"âœ… {test_name} æµ‹è¯•é€šè¿‡")
            else:
                print(f"âŒ {test_name} æµ‹è¯•å¤±è´¥")
        except Exception as e:
            print(f"âŒ {test_name} æµ‹è¯•å¼‚å¸¸: {e}")
    
    print(f"\nğŸ“Š æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æ ¸å¿ƒæ–‡æœ¬å¤„ç†åŠŸèƒ½æµ‹è¯•é€šè¿‡!")
        print("\nğŸ“‹ åŠŸèƒ½å®ç°æ€»ç»“:")
        print("   âœ… æ–‡æœ¬æå– (PDF, DOCX, HTML)")
        print("   âœ… æ–‡æœ¬æ¸…ç†å’Œé¢„å¤„ç†")
        print("   âœ… æ™ºèƒ½æ–‡æœ¬åˆ†å—")
        print("   âœ… Tokenæ•°é‡ä¼°ç®—")
        print("   âœ… å…³é”®è¯æå–")
        print("   âœ… æ–‡æœ¬å—è´¨é‡éªŒè¯")
        print("   âœ… å¼‚æ­¥å¤„ç†æ¡†æ¶")
        print("   âœ… é…ç½®ç®¡ç†")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®ç°")

if __name__ == "__main__":
    main() 