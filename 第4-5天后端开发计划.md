# 第4-5天：文献查看 & AI助手集成开发计划

## 当前状态分析
✅ **已完成**：
- 用户认证系统（登录、JWT令牌、bcrypt密码哈希）
- 研究组管理（创建、加入、邀请码机制）
- 公共文献库（文件上传、存储、列表查看、软删除）
- 数据库模型：User、ResearchGroup、UserResearchGroup、Literature
- 文件存储系统（按研究组分目录、权限控制）

## 目标功能
实现文献文件查看服务和AI助手的RAG问答功能，优先实现基本框架，复杂功能简化处理。

---

## 第4天：文献文件服务 & AI基础搭建

### 阶段1：文献文件服务接口（优先级：⭐⭐⭐必须完成）

#### 1.1 文件查看接口实现
- [ ] **在 `app/main.py` 添加文件服务接口**
  - [ ] 路由：`GET /literature/view/file/{literature_id}`
  - [ ] 功能：提供文献文件的安全下载和流式传输
  - [ ] 权限验证：
    - 用户已登录
    - 用户是该文献所属研究组成员
    - 文献状态为active（未被软删除）
  - [ ] 处理流程：
    1. 验证literature_id有效性
    2. 检查用户组成员身份
    3. 获取文件路径并验证文件存在
    4. 根据文件类型设置正确的Content-Type
    5. 使用FastAPI的FileResponse返回文件
  - [ ] 支持的响应类型：
    - PDF: `application/pdf`
    - DOCX: `application/vnd.openxmlformats-officedocument.wordprocessingml.document`
    - HTML: `text/html`

#### 1.2 文件元数据接口
- [ ] **在 `app/main.py` 添加文献详情接口**
  - [ ] 路由：`GET /literature/detail/{literature_id}`
  - [ ] 返回文献的详细信息（标题、文件名、大小、上传时间等）
  - [ ] 权限验证：同文件查看接口
  - [ ] 用于前端显示文献信息和决定如何处理文件

#### 1.3 增强权限验证工具
- [ ] **更新 `app/utils/auth_helper.py`**
  - [ ] 添加 `verify_literature_access(user_id, literature_id, db)`: 验证用户对特定文献的访问权限
  - [ ] 添加 `get_literature_with_permission(literature_id, user_id, db)`: 获取文献信息并验证权限
  - [ ] 优化错误处理：区分文献不存在、权限不足、文献已删除等情况

### 阶段2：AI依赖配置 & 环境准备（优先级：⭐⭐⭐必须完成）

#### 2.1 安装AI相关依赖
- [ ] **更新 `requirements.txt`**
  - [ ] 添加 `langchain>=0.1.0`
  - [ ] 添加 `openai>=1.0.0`
  - [ ] 添加 `tiktoken>=0.5.0` (OpenAI tokenizer)
  - [ ] 添加 `chromadb>=0.4.0` (向量数据库)
  - [ ] 添加 `sentence-transformers>=2.2.0` (备用embedding模型)

#### 2.2 AI配置管理
- [ ] **更新 `app/config.py`**
  - [ ] 添加AI相关配置：
    - `OPENAI_API_KEY`: OpenAI API密钥
    - `EMBEDDING_MODEL`: embedding模型名称（默认text-embedding-3-small）
    - `LLM_MODEL`: 聊天模型名称（默认gpt-3.5-turbo）
    - `CHUNK_SIZE`: 文本分块大小（默认1000）
    - `CHUNK_OVERLAP`: 分块重叠大小（默认200）
    - `VECTOR_DB_PATH`: 向量数据库存储路径（./vector_db/）
    - `MAX_RETRIEVAL_DOCS`: 最大检索文档数（默认5）

#### 2.3 环境变量配置
- [ ] **创建 `.env.example`**
  - [ ] 添加必要的环境变量示例
  - [ ] 更新 `.gitignore` 确保 `.env` 不被提交
- [ ] **配置OpenAI API密钥**
  - [ ] 在 `.env` 文件中设置 `OPENAI_API_KEY`
  - [ ] 在config.py中读取环境变量

### 阶段3：文本处理 & 分块功能（优先级：⭐⭐重要）

#### 3.1 增强文本提取功能
- [ ] **更新 `app/utils/text_extractor.py`**
  - [ ] 优化 `extract_pdf_text()`: 添加错误处理和文本清理
  - [ ] 添加 `extract_docx_text()`: 使用python-docx提取Word文档文本
  - [ ] 添加 `extract_html_text()`: 使用BeautifulSoup提取HTML文本
  - [ ] 添加 `clean_extracted_text()`: 清理提取的文本（去除多余空白、特殊字符等）
  - [ ] **简化策略**：如果提取失败，返回空字符串而不是抛出异常

#### 3.2 文本分块处理
- [ ] **创建 `app/utils/text_processor.py`**
  - [ ] `split_text_into_chunks(text, chunk_size, overlap)`: 使用Langchain的RecursiveCharacterTextSplitter
  - [ ] `prepare_chunks_for_embedding(chunks, literature_id, group_id)`: 为每个chunk添加元数据
  - [ ] `estimate_token_count(text)`: 使用tiktoken估算token数量
  - [ ] **简化实现**：
    - 使用固定的分块策略，不做复杂的语义分块
    - 暂不处理表格、图片等特殊内容
    - 分块时保留基本的段落结构

#### 3.3 异步文本处理任务
- [ ] **创建 `app/utils/async_processor.py`**
  - [ ] `process_literature_async(literature_id)`: 异步处理文献文本
  - [ ] 处理流程：
    1. 从数据库获取文献信息
    2. 提取文本内容
    3. 分块处理
    4. 生成embedding（下一阶段实现）
    5. 存储到向量数据库
  - [ ] **简化实现**：使用Python的threading模块，不引入复杂的任务队列

### 阶段4：向量数据库基础（优先级：⭐⭐重要）

#### 4.1 ChromaDB集成
- [ ] **创建 `app/utils/vector_store.py`**
  - [ ] `initialize_chroma_client()`: 初始化ChromaDB客户端
  - [ ] `create_collection_for_group(group_id)`: 为研究组创建向量集合
  - [ ] `get_or_create_collection(group_id)`: 获取或创建集合
  - [ ] **简化配置**：使用ChromaDB的默认持久化设置，不做复杂的配置优化

#### 4.2 Embedding生成服务
- [ ] **创建 `app/utils/embedding_service.py`**
  - [ ] `generate_embeddings(texts)`: 使用OpenAI API生成embeddings
  - [ ] `batch_generate_embeddings(text_chunks)`: 批量生成embeddings（节省API调用）
  - [ ] 添加错误处理和重试机制
  - [ ] **简化实现**：直接调用OpenAI API，不做复杂的缓存和优化

#### 4.3 向量存储操作
- [ ] **在 `app/utils/vector_store.py` 添加存储功能**
  - [ ] `store_document_chunks(chunks, embeddings, literature_id, group_id)`: 存储文档块到向量数据库
  - [ ] `delete_document_chunks(literature_id)`: 删除文献对应的所有向量
  - [ ] `search_similar_chunks(query_embedding, group_id, literature_id, top_k)`: 相似度搜索
  - [ ] 每个chunk的元数据包含：literature_id、group_id、chunk_index、original_text

---

## 第5天：RAG问答管道 & AI接口实现

### 阶段5：RAG问答核心功能（优先级：⭐⭐⭐必须完成）

#### 5.1 问答接口实现
- [ ] **在 `app/main.py` 添加AI问答接口**
  - [ ] 路由：`POST /ai/ask`
  - [ ] 请求参数：
    - `question: str` (用户问题)
    - `literature_id: str` (当前查看的文献ID)
    - `conversation_history: Optional[List[Dict]]` (对话历史，可选)
  - [ ] 权限验证：
    - 用户已登录
    - 用户有权访问指定文献
  - [ ] 响应格式：
    - `answer: str` (AI回答)
    - `sources: List[str]` (引用来源)
    - `confidence: float` (置信度，可选)

#### 5.2 RAG检索链实现
- [ ] **创建 `app/utils/rag_service.py`**
  - [ ] `retrieve_relevant_chunks(question, literature_id, group_id, top_k)`: 检索相关文档块
  - [ ] 处理流程：
    1. 将用户问题转换为embedding
    2. 在指定文献的向量中进行相似度搜索
    3. 返回最相关的文档块和相似度分数
  - [ ] **简化实现**：使用简单的余弦相似度，不做复杂的重排序

#### 5.3 RAG生成链实现
- [ ] **在 `app/utils/rag_service.py` 添加生成功能**
  - [ ] `generate_answer(question, relevant_chunks, conversation_history)`: 生成AI回答
  - [ ] 提示词模板设计：
    - 系统提示：定义AI助手的角色和行为规范
    - 上下文：包含检索到的相关文档块
    - 对话历史：最近的几轮对话
    - 用户问题：当前问题
  - [ ] 要求AI在回答中标注引用来源
  - [ ] **简化实现**：使用固定的提示词模板，不做动态优化

#### 5.4 对话历史管理
- [ ] **创建 `app/utils/conversation_manager.py`**
  - [ ] `format_conversation_history(history)`: 格式化对话历史为LLM可理解的格式
  - [ ] `truncate_conversation(history, max_tokens)`: 截断过长的对话历史
  - [ ] `extract_key_context(history)`: 提取对话中的关键上下文
  - [ ] **简化实现**：只保留最近5轮对话，不做复杂的上下文压缩

### 阶段6：AI接口优化 & 错误处理（优先级：⭐⭐重要）

#### 6.1 预设问题功能
- [ ] **在 `app/main.py` 添加预设问题接口**
  - [ ] 路由：`GET /ai/preset-questions/{literature_id}`
  - [ ] 返回针对当前文献的预设问题列表：
    - "请总结这篇文献的核心论点"
    - "这篇文献的主要研究方法是什么？"
    - "文献中有哪些重要的实验结果？"
    - "这篇文献的创新点在哪里？"
    - "文献中提到了哪些局限性？"
  - [ ] **简化实现**：返回固定的问题列表，不做个性化推荐

#### 6.2 AI服务错误处理
- [ ] **在 `app/utils/rag_service.py` 添加错误处理**
  - [ ] OpenAI API调用失败的处理
  - [ ] 向量数据库连接失败的处理
  - [ ] 文本提取失败的处理
  - [ ] 超时处理和重试机制
  - [ ] 优雅降级：当AI服务不可用时，返回友好的错误信息

#### 6.3 性能优化
- [ ] **添加基础缓存机制**
  - [ ] 缓存常见问题的embedding
  - [ ] 缓存文献的文本块（避免重复提取）
  - [ ] **简化实现**：使用Python字典作为内存缓存，不引入Redis

#### 6.4 API响应优化
- [ ] **优化AI接口响应时间**
  - [ ] 添加流式响应支持（可选）
  - [ ] 添加请求状态跟踪
  - [ ] 设置合理的超时时间
  - [ ] 添加响应时间监控日志

### 阶段7：集成测试 & 验证（优先级：⭐⭐重要）

#### 7.1 端到端测试
- [ ] **创建 `test_ai_integration.py`**
  - [ ] 测试文献文件查看功能
  - [ ] 测试AI问答完整流程
  - [ ] 测试权限控制
  - [ ] 测试错误场景处理
  - [ ] **简化测试**：手动测试主要场景，记录测试结果

#### 7.2 数据库更新脚本
- [ ] **创建 `update_existing_literature.py`**
  - [ ] 为现有文献生成文本块和embedding
  - [ ] 批量处理已上传的文献
  - [ ] 添加进度显示和错误处理

#### 7.3 系统健康检查
- [ ] **在 `app/main.py` 添加健康检查接口**
  - [ ] 路由：`GET /health/ai`
  - [ ] 检查项目：
    - OpenAI API连接状态
    - ChromaDB连接状态
    - 向量数据库数据统计
    - AI服务响应时间
  - [ ] 返回系统健康状态和详细信息

---

## 技术简化策略

### AI相关简化
- **向量数据库**：使用ChromaDB默认配置，不做复杂的索引优化
- **文本分块**：使用固定大小分块，不做语义分块
- **Embedding模型**：直接使用OpenAI API，不部署本地模型
- **LLM调用**：直接调用OpenAI API，不做模型微调
- **对话管理**：简单的历史截断，不做复杂的上下文管理

### 性能相关简化
- **缓存**：使用内存缓存，不引入Redis
- **异步处理**：使用Python threading，不引入Celery
- **监控**：基础日志记录，不引入复杂的监控系统
- **负载均衡**：单实例部署，不考虑分布式

### 功能相关简化
- **文件格式**：优先支持PDF，Word/HTML基础支持
- **多语言**：只支持中英文，不做国际化
- **个性化**：固定的预设问题，不做用户偏好学习
- **高级检索**：基础相似度搜索，不做复杂的检索策略

---

## 实施建议

### 第4天优先级
1. **必须完成（⭐⭐⭐）**：
   - 文献文件服务接口
   - AI依赖配置和环境准备
   - 基础文本处理功能

2. **重要（⭐⭐）**：
   - 文本分块和向量数据库基础
   - 异步处理框架

3. **可选（⭐）**：
   - 性能优化和缓存

### 第5天优先级
1. **必须完成（⭐⭐⭐）**：
   - RAG问答核心功能
   - AI接口实现
   - 基础错误处理

2. **重要（⭐⭐）**：
   - 预设问题功能
   - 集成测试验证

3. **可选（⭐）**：
   - 高级优化功能

### 风险控制
- **API密钥安全**：确保OpenAI API密钥不被泄露
- **成本控制**：设置API调用限制，避免过度消费
- **错误恢复**：确保AI服务故障不影响基础功能
- **数据隔离**：确保不同研究组的数据严格隔离

---

## 预计时间分配

### 第4天（8小时）
- **阶段1**: 文献文件服务（2小时）
- **阶段2**: AI依赖配置（1.5小时）
- **阶段3**: 文本处理功能（2.5小时）
- **阶段4**: 向量数据库基础（2小时）

### 第5天（8小时）
- **阶段5**: RAG问答核心（4小时）
- **阶段6**: AI接口优化（2小时）
- **阶段7**: 集成测试验证（2小时）

**总计**: 16小时（两个工作日）

---

## 成功标准

完成本计划后，应该能够：

### 第4天成功标准
1. ✅ 用户可以安全地查看和下载研究组内的文献文件
2. ✅ 系统能够提取文献文本并进行分块处理
3. ✅ AI环境配置完成，可以调用OpenAI API
4. ✅ 向量数据库可以存储和检索文档块

### 第5天成功标准
1. ✅ 用户可以对当前查看的文献进行AI问答
2. ✅ AI能够基于文献内容生成准确回答并标注来源
3. ✅ 系统支持多轮对话和预设问题
4. ✅ 具备完整的错误处理和优雅降级机制
5. ✅ 通过端到端测试验证所有功能正常

### 整体目标
- **技术目标**：建立完整的RAG问答管道
- **用户体验**：提供流畅的文献查看和AI交互体验
- **系统稳定性**：确保AI功能不影响基础功能的稳定性
- **可扩展性**：为后续功能扩展奠定良好基础

这些功能将为第6-7天的前端集成和系统优化奠定坚实基础。