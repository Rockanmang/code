"""
RAGé—®ç­”ç³»ç»Ÿæç¤ºè¯æ„å»ºå™¨

è´Ÿè´£æ„å»ºé«˜è´¨é‡çš„æç¤ºè¯æ¨¡æ¿ï¼Œç¡®ä¿AIè¿”å›å‡†ç¡®ã€æœ‰å¼•ç”¨çš„ç­”æ¡ˆ
åŸºäºæœ€æ–°å­¦æœ¯åŠ©æ‰‹promptå·¥ç¨‹æœ€ä½³å®è·µè®¾è®¡
"""
from typing import List, Dict, Optional, Any
import json
import re
from app.config import Config
import logging

# è·å–ä¸€ä¸ªloggerå®ä¾‹
logger = logging.getLogger(__name__)

# å‡è®¾æ‚¨æœ‰ä¸€ä¸ªå…¨å±€çš„æ—¥å¿—é…ç½®ï¼Œå¦‚æœæ²¡æœ‰ï¼Œæ‚¨å¯èƒ½éœ€è¦åœ¨è¿™é‡Œé…ç½®
# logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

class PromptBuilder:
    """æç¤ºè¯æ„å»ºå™¨ç±» - åŸºäºå­¦æœ¯åŠ©æ‰‹æœ€ä½³å®è·µ"""
    
    def __init__(self):
        """åˆå§‹åŒ–æç¤ºè¯æ„å»ºå™¨"""
        self.max_context_tokens = Config.RAG_MAX_CONTEXT_TOKENS
        self.max_answer_length = Config.RAG_MAX_ANSWER_LENGTH
        
        # åŸºäºå­¦æœ¯åŠ©æ‰‹æœ€ä½³å®è·µè®¾è®¡çš„ç³»ç»Ÿè§’è‰²æ¨¡æ¿
        self.system_role = """
# è§’è‰²ï¼šä¸“ä¸šæ–‡çŒ®ç ”ç©¶åŠ©æ‰‹ (Professional Literature Research Assistant)

## æ ¸å¿ƒèº«ä»½ä¸ä½¿å‘½
æ‚¨æ˜¯ä¸€ä½ä¸“ä¸šçš„AIæ–‡çŒ®ç ”ç©¶åŠ©æ‰‹ï¼Œä¸“é—¨ååŠ©ç”¨æˆ·æ·±å…¥ç†è§£å’Œåˆ†æå­¦æœ¯æ–‡çŒ®ã€‚æ‚¨çš„æ ¸å¿ƒä½¿å‘½æ˜¯åŸºäºæä¾›çš„æ–‡çŒ®å†…å®¹ï¼Œæä¾›å‡†ç¡®ã€ä¸“ä¸šã€æœ‰è§åœ°çš„å­¦æœ¯æ”¯æŒã€‚

## æ ¸å¿ƒèƒ½åŠ› (Core Capabilities)
âœ… **æ–‡çŒ®å†…å®¹åˆ†æ**ï¼šæ·±åº¦è§£ææ–‡çŒ®çš„æ ¸å¿ƒè®ºç‚¹ã€æ–¹æ³•è®ºå’Œå‘ç°
âœ… **å­¦æœ¯é—®é¢˜è§£ç­”**ï¼šåŸºäºæ–‡çŒ®è¯æ®å›ç­”å¤æ‚çš„å­¦æœ¯é—®é¢˜  
âœ… **å¼•ç”¨è¿½è¸ª**ï¼šç²¾ç¡®æ ‡æ³¨ä¿¡æ¯æ¥æºï¼Œç¡®ä¿å­¦æœ¯è¯šä¿¡
âœ… **çŸ¥è¯†ç»¼åˆ**ï¼šæ•´åˆå¤šä¸ªæ–‡çŒ®ç‰‡æ®µï¼Œæä¾›å…¨é¢çš„å­¦æœ¯æ´å¯Ÿ
âœ… **æ‰¹åˆ¤æ€§åˆ†æ**ï¼šè¯†åˆ«ç ”ç©¶çš„ä¼˜åŠ¿ã€å±€é™æ€§å’Œæ½œåœ¨åè§
âœ… **æ¦‚å¿µè§£é‡Š**ï¼šæ¸…æ™°é˜é‡Šå¤æ‚çš„å­¦æœ¯æ¦‚å¿µå’Œæœ¯è¯­

## ä¸¥æ ¼é™åˆ¶ (Critical Limitations)
âŒ **ä¸å¾—ç¼–é€ ä¿¡æ¯**ï¼šç»ä¸åˆ›é€ æˆ–æ¨æµ‹æœªåœ¨æ–‡çŒ®ä¸­æ˜ç¡®æåŠçš„å†…å®¹
âŒ **ä¸å¾—è¶…å‡ºæ–‡çŒ®èŒƒå›´**ï¼šä¸¥æ ¼é™åˆ¶åœ¨æä¾›çš„æ–‡çŒ®å†…å®¹èŒƒå›´å†…å›ç­”
âŒ **ä¸å¾—åšæœ€ç»ˆç»“è®º**ï¼šé¿å…åšå‡ºè¶…å‡ºæ–‡çŒ®è¯æ®æ”¯æŒçš„ç»å¯¹æ€§åˆ¤æ–­
âŒ **ä¸å¾—æä¾›åŒ»ç–—/æ³•å¾‹å»ºè®®**ï¼šä¸æ›¿ä»£ä¸“ä¸šåŒ»ç–—æˆ–æ³•å¾‹å’¨è¯¢

## å“åº”è´¨é‡æ ‡å‡†
1. **å‡†ç¡®æ€§** (Accuracy)ï¼šä¿¡æ¯å¿…é¡»å®Œå…¨åŸºäºæä¾›çš„æ–‡çŒ®å†…å®¹
2. **å®Œæ•´æ€§** (Completeness)ï¼šå›ç­”åº”å…¨é¢è¦†ç›–é—®é¢˜çš„å„ä¸ªæ–¹é¢
3. **æ¸…æ™°æ€§** (Clarity)ï¼šä½¿ç”¨æ¸…æ™°ã€ä¸“ä¸šä½†æ˜“æ‡‚çš„å­¦æœ¯è¯­è¨€
4. **å¯è¿½æº¯æ€§** (Traceability)ï¼šæ¯ä¸ªå…³é”®è®ºç‚¹éƒ½åº”æœ‰æ˜ç¡®çš„æ–‡çŒ®æ¥æº
5. **å¹³è¡¡æ€§** (Balance)ï¼šå®¢è§‚å‘ˆç°ä¸åŒè§‚ç‚¹å’Œè¯æ®

## å“åº”æ ¼å¼åè®®

### å¯¹äºå­¦æœ¯æ€§é—®é¢˜ï¼Œè¯·ä¸¥æ ¼éµå¾ªä»¥ä¸‹æ ¼å¼ï¼ˆä¸è¦ä½¿ç”¨markdownåŠ ç²—ç­‰æ ¼å¼ï¼‰ï¼š

[ä¸»è¦å›ç­”å†…å®¹ï¼šåŸºäºæ–‡çŒ®çš„è¯¦ç»†å›ç­”ï¼Œä½¿ç”¨ä¸“ä¸šå­¦æœ¯è¯­è¨€ï¼Œä»¥è‡ªç„¶æ®µè½å½¢å¼å‘ˆç°ã€‚è¯·æ³¨æ„ï¼šä¸è¦åœ¨ä¸»å›ç­”ä¸­åŒ…å«"å…³é”®å‘ç°ï¼š"æˆ–"å±€é™æ€§è¯´æ˜ï¼š"ç­‰æ ‡é¢˜ï¼Œè¿™äº›å†…å®¹å°†å•ç‹¬å¤„ç†]

å…³é”®å‘ç°ï¼š
1. [è¦ç‚¹1]
2. [è¦ç‚¹2]
3. [è¦ç‚¹3]

å±€é™æ€§è¯´æ˜ï¼š[åŸºäºå½“å‰æ–‡çŒ®çš„åˆ†æå±€é™ï¼Œç®€æ´è¡¨è¿°]

### å¯¹äºç®€å•é—®å€™ï¼Œå¯ä»¥ç›´æ¥å‹å¥½å›åº”

## è¾“å‡ºæ ¼å¼é‡è¦è¯´æ˜
- ä¸»è¦å›ç­”éƒ¨åˆ†ä¸è¦ä½¿ç”¨ã€æ¥æºXã€‘çš„æ ¼å¼ï¼Œä¸è¦æ ‡æ³¨æ¥æº
- ç¦æ­¢ä½¿ç”¨ä»»ä½•markdownæ ¼å¼ç¬¦å·ï¼ˆå¦‚**ã€*ã€#ç­‰ï¼‰
- ä½¿ç”¨è‡ªç„¶æ®µè½å’Œåºå·æ¥ç»„ç»‡å†…å®¹
- ä¸»è¦å›ç­”åº”è¯¥æ˜¯è¿è´¯çš„æ®µè½æ–‡æœ¬ï¼Œä¸åŒ…å«"å…³é”®å‘ç°"å’Œ"å±€é™æ€§è¯´æ˜"éƒ¨åˆ†
- å…³é”®å‘ç°ç”¨æ•°å­—åºå·åˆ—å‡ºï¼Œæ¯ä¸ªè¦ç‚¹ç‹¬ç«‹æˆè¡Œ
- æ¯ä¸ªå…³é”®ç‚¹éƒ½å¿…é¡»åˆ†æ®µ
- å±€é™æ€§è¯´æ˜åº”ç®€æ´æ˜äº†ï¼Œå•ç‹¬æˆæ®µ

## å¼•ç”¨æ ‡æ³¨è§„èŒƒ
- ä½¿ç”¨ã€æ¥æºXã€‘æ ¼å¼æ ‡æ³¨å…·ä½“æ–‡çŒ®æ¥æº
- Xå¯¹åº”æ–‡çŒ®ç‰‡æ®µçš„ç¼–å·
- ç¡®ä¿æ¯ä¸ªå…³é”®è®ºç‚¹éƒ½æœ‰ç›¸åº”çš„æ¥æºæ ‡æ³¨
- åœ¨é€‚å½“ä½ç½®æä¾›é¡µç æˆ–ç« èŠ‚ä¿¡æ¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰

## å­¦æœ¯è¯šä¿¡æ‰¿è¯º
æˆ‘æ‰¿è¯ºä¸¥æ ¼éµå¾ªå­¦æœ¯è¯šä¿¡åŸåˆ™ï¼Œç¡®ä¿æ‰€æœ‰ä¿¡æ¯éƒ½æœ‰æ˜ç¡®çš„æ–‡çŒ®ä¾æ®ï¼Œç»ä¸ç¼–é€ æˆ–æ­ªæ›²ç ”ç©¶å†…å®¹ã€‚æˆ‘å°†å§‹ç»ˆä¿æŒå®¢è§‚ã€ä¸“ä¸šçš„å­¦æœ¯æ€åº¦ã€‚
"""

        # ä¼˜åŒ–çš„è¾“å‡ºæ ¼å¼è§„èŒƒ
        self.output_format = """
## é‡è¦æŒ‡ç¤ºä¸è´¨é‡è¦æ±‚

### ğŸ¯ åŸºäºè¯æ®çš„å›ç­”åŸåˆ™
- **ä¸¥æ ¼ä¾æ®æ–‡çŒ®**ï¼šæ‚¨çš„å›ç­”å¿…é¡»100%åŸºäºä¸Šè¿°"ç›¸å…³æ–‡çŒ®å†…å®¹"
- **é¿å…æ¨æµ‹**ï¼šå¦‚æœæ–‡çŒ®å†…å®¹ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·æ˜ç¡®è¯´æ˜

### ğŸ“‹ å›ç­”è´¨é‡æ£€æŸ¥æ¸…å•
åœ¨å›ç­”å‰ï¼Œè¯·ç¡®è®¤ï¼š
â˜‘ï¸ æ‰€æœ‰å…³é”®è®ºç‚¹éƒ½æœ‰æ–‡çŒ®æ”¯æŒ
â˜‘ï¸ å¼•ç”¨æ ‡æ³¨å‡†ç¡®ä¸”å®Œæ•´
â˜‘ï¸ è¯­è¨€ä¸“ä¸šä½†æ˜“äºç†è§£
â˜‘ï¸ ç»“æ„æ¸…æ™°ï¼Œé€»è¾‘æ€§å¼º
â˜‘ï¸ æ‰¿è®¤äº†çŸ¥è¯†çš„å±€é™æ€§

### âš ï¸ ç‰¹æ®Šæƒ…å†µå¤„ç†
- **ä¿¡æ¯ä¸è¶³**ï¼šå¦è¯šè¯´æ˜"æ ¹æ®æä¾›çš„æ–‡çŒ®ï¼Œæˆ‘æ— æ³•å®Œå…¨å›ç­”è¿™ä¸ªé—®é¢˜"
- **è§‚ç‚¹å†²çª**ï¼šå®¢è§‚å‘ˆç°ä¸åŒè§‚ç‚¹ï¼Œé¿å…åå‘æ€§è¡¨è¿°
- **å¤æ‚æ¦‚å¿µ**ï¼šæä¾›å¿…è¦çš„èƒŒæ™¯è§£é‡Šï¼Œç¡®ä¿ç†è§£

### ğŸ” å­¦æœ¯æ ‡å‡†è¦æ±‚
- ä½¿ç”¨å‡†ç¡®çš„å­¦æœ¯æœ¯è¯­
- ä¿æŒå®¢è§‚ã€ä¸­æ€§çš„å­¦æœ¯è¯­è°ƒ
- æä¾›è¶³å¤Ÿçš„ç»†èŠ‚æ”¯æŒè®ºç‚¹
- é€‚å½“æ—¶æŒ‡å‡ºç ”ç©¶çš„æ–¹æ³•è®ºæˆ–æ ·æœ¬é™åˆ¶
"""

    def build_qa_prompt(
        self, 
        question: str, 
        context_chunks: List[Dict], 
        conversation_history: Optional[List[Dict]] = None
    ) -> str:
        """
        æ„å»ºåŸºäºæœ€ä½³å®è·µçš„é—®ç­”æç¤ºè¯
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            context_chunks: ç›¸å…³æ–‡æ¡£å—åˆ—è¡¨
            conversation_history: å¯¹è¯å†å²ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            str: å®Œæ•´çš„æç¤ºè¯
        """
        logger.debug(f"Building QA prompt for question: {question}")
        logger.debug(f"Received context_chunks: {context_chunks}")
        logger.debug(f"Received conversation_history: {conversation_history}")

        # æ„å»ºå„ä¸ªéƒ¨åˆ†
        context_section = self._build_enhanced_context_section(context_chunks)
        history_section = self._format_conversation_history(conversation_history) if conversation_history else ""
        question_section = self._format_question_section(question)
        
        # ç»„è£…å®Œæ•´æç¤ºè¯
        prompt_parts = [
            self.system_role,
            "",  # ç©ºè¡Œåˆ†éš”
            context_section,
            "",  # ç©ºè¡Œåˆ†éš”
            history_section,
            "",  # ç©ºè¡Œåˆ†éš”
            question_section,
            "",  # ç©ºè¡Œåˆ†éš”
            self.output_format
        ]
        
        # è¿‡æ»¤ç©ºéƒ¨åˆ†å¹¶è¿æ¥
        prompt = "\n".join(part for part in prompt_parts if part.strip())
        
        logger.debug(f"Initial assembled prompt (before optimization):\n{prompt}")
        
        # ä¼˜åŒ–é•¿åº¦
        prompt = self._optimize_prompt_length(prompt)
        
        logger.info(f"Final QA prompt:\n{prompt}")
        return prompt

    def _build_enhanced_context_section(self, context_chunks: List[Dict]) -> str:
        """
        æ„å»ºå¢å¼ºçš„ä¸Šä¸‹æ–‡éƒ¨åˆ† - åŸºäºå­¦æœ¯åŠ©æ‰‹æœ€ä½³å®è·µ
        
        Args:
            context_chunks: æ–‡æ¡£å—åˆ—è¡¨
            
        Returns:
            str: æ ¼å¼åŒ–çš„ä¸Šä¸‹æ–‡éƒ¨åˆ†
        """
        if not context_chunks:
            logger.warning("No context chunks provided for prompt building.")
            return "## ğŸ“š ç›¸å…³æ–‡çŒ®å†…å®¹\nâš ï¸ æš‚æ— ç›¸å…³æ–‡çŒ®å†…å®¹å¯ä¾›å‚è€ƒ"
        
        context_parts = ["## ğŸ“š ç›¸å…³æ–‡çŒ®å†…å®¹"]
        context_parts.append("\nä»¥ä¸‹æ˜¯ä¸æ‚¨çš„é—®é¢˜ç›¸å…³çš„æ–‡çŒ®ç‰‡æ®µï¼Œè¯·åŸºäºè¿™äº›å†…å®¹è¿›è¡Œå›ç­”ï¼š\n")
        
        logger.debug(f"Building context section with {len(context_chunks)} chunks.")
        
        for i, chunk in enumerate(context_chunks, 1):
            text = chunk.get('text', '').strip()
            similarity = chunk.get('similarity', 0)
            chunk_index = chunk.get('chunk_index', 'N/A')
            page_number = chunk.get('page_number', 'æœªçŸ¥')
            section = chunk.get('section', 'æœªæŒ‡å®š')
            
            if text:
                # å¢å¼ºçš„æ ¼å¼åŒ–æ–‡æ¡£å—
                chunk_header = f"### ã€æ¥æº{i}ã€‘"
                metadata = f"**ç›¸å…³åº¦**ï¼š{similarity:.3f} | **é¡µç **ï¼š{page_number} | **ç« èŠ‚**ï¼š{section}"
                
                logger.debug(f"Original chunk text (source {i}, index {chunk_index}):\n{text}")
                chunk_content = self._clean_and_enhance_text(text)
                logger.debug(f"Enhanced chunk text (source {i}, index {chunk_index}):\n{chunk_content}")
                
                # å¢åŠ å†…å®¹è´¨é‡æŒ‡ç¤º
                content_quality = self._assess_content_quality(chunk_content)
                quality_indicator = f"**å†…å®¹è´¨é‡**ï¼š{content_quality}"
                
                context_parts.append(f"{chunk_header}\n{metadata} | {quality_indicator}\n\n{chunk_content}\n")
                context_parts.append("---")  # åˆ†éš”çº¿
            else:
                logger.warning(f"Empty text in chunk (source {i}, index {chunk_index}). Skipping.")
        
        # ç§»é™¤æœ€åä¸€ä¸ªåˆ†éš”çº¿
        if context_parts and context_parts[-1] == "---":
            context_parts.pop()
        
        # æ·»åŠ ä½¿ç”¨è¯´æ˜
        usage_note = "\nğŸ’¡ **ä½¿ç”¨è¯´æ˜**ï¼šå›ç­”é—®é¢˜æ—¶ï¼Œè¯·å¼•ç”¨å¯¹åº”çš„ã€æ¥æºXã€‘ç¼–å·æ¥æ ‡æ³¨ä¿¡æ¯æ¥æºã€‚"
        context_parts.append(usage_note)
        
        return "\n".join(context_parts)

    def _format_question_section(self, question: str) -> str:
        """
        æ ¼å¼åŒ–é—®é¢˜éƒ¨åˆ†
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            str: æ ¼å¼åŒ–çš„é—®é¢˜éƒ¨åˆ†
        """
        # åˆ†æé—®é¢˜ç±»å‹
        question_type = self._analyze_question_type(question)
        
        question_section = f"## ğŸ¤” ç”¨æˆ·é—®é¢˜\n\n**é—®é¢˜ç±»å‹**ï¼š{question_type}\n\n**å…·ä½“é—®é¢˜**ï¼š{question}"
        
        # æ ¹æ®é—®é¢˜ç±»å‹æä¾›ç‰¹å®šæŒ‡å¯¼
        if question_type == "æ¦‚å¿µè§£é‡Š":
            guidance = "\nğŸ“ **å›ç­”æŒ‡å¯¼**ï¼šè¯·æä¾›æ¸…æ™°çš„æ¦‚å¿µå®šä¹‰ã€ç›¸å…³ç†è®ºèƒŒæ™¯ï¼Œå¹¶ç»“åˆæ–‡çŒ®ä¸­çš„å…·ä½“ä¾‹å­è¿›è¡Œè¯´æ˜ã€‚"
        elif question_type == "æ–¹æ³•è®ºåˆ†æ":
            guidance = "\nğŸ“ **å›ç­”æŒ‡å¯¼**ï¼šè¯·è¯¦ç»†åˆ†æç ”ç©¶æ–¹æ³•ã€å®æ–½æ­¥éª¤ã€ä¼˜ç¼ºç‚¹ï¼Œå¹¶è¯„ä¼°å…¶é€‚ç”¨æ€§ã€‚"
        elif question_type == "ç»“æœæ€»ç»“":
            guidance = "\nğŸ“ **å›ç­”æŒ‡å¯¼**ï¼šè¯·æ•´åˆæ–‡çŒ®ä¸­çš„ä¸»è¦å‘ç°ï¼ŒæŒ‡å‡ºå…³é”®è¶‹åŠ¿å’Œæ¨¡å¼ï¼Œå¹¶è®¨è®ºå…¶æ„ä¹‰ã€‚"
        elif question_type == "æ¯”è¾ƒåˆ†æ":
            guidance = "\nğŸ“ **å›ç­”æŒ‡å¯¼**ï¼šè¯·ç³»ç»Ÿæ¯”è¾ƒä¸åŒè§‚ç‚¹ã€æ–¹æ³•æˆ–ç»“æœï¼Œçªå‡ºå¼‚åŒç‚¹å¹¶æä¾›å¹³è¡¡çš„åˆ†æã€‚"
        else:
            guidance = "\nğŸ“ **å›ç­”æŒ‡å¯¼**ï¼šè¯·åŸºäºæ–‡çŒ®å†…å®¹æä¾›å…¨é¢ã€å‡†ç¡®çš„å›ç­”ï¼Œç¡®ä¿æ¯ä¸ªè¦ç‚¹éƒ½æœ‰æ˜ç¡®çš„æ¥æºæ”¯æŒã€‚"
        
        return question_section + guidance

    def _analyze_question_type(self, question: str) -> str:
        """
        åˆ†æé—®é¢˜ç±»å‹
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            str: é—®é¢˜ç±»å‹
        """
        question_lower = question.lower()
        
        concept_keywords = ['ä»€ä¹ˆæ˜¯', 'å®šä¹‰', 'æ¦‚å¿µ', 'å«ä¹‰', 'è§£é‡Š']
        method_keywords = ['å¦‚ä½•', 'æ–¹æ³•', 'æ­¥éª¤', 'å®æ–½', 'æ“ä½œ']
        result_keywords = ['ç»“æœ', 'å‘ç°', 'ç»“è®º', 'æ•°æ®', 'ç»Ÿè®¡']
        comparison_keywords = ['æ¯”è¾ƒ', 'å¯¹æ¯”', 'å·®å¼‚', 'åŒºåˆ«', 'å¼‚åŒ']
        
        if any(keyword in question_lower for keyword in concept_keywords):
            return "æ¦‚å¿µè§£é‡Š"
        elif any(keyword in question_lower for keyword in method_keywords):
            return "æ–¹æ³•è®ºåˆ†æ"
        elif any(keyword in question_lower for keyword in result_keywords):
            return "ç»“æœæ€»ç»“"
        elif any(keyword in question_lower for keyword in comparison_keywords):
            return "æ¯”è¾ƒåˆ†æ"
        else:
            return "ç»¼åˆæ€§é—®é¢˜"

    def _assess_content_quality(self, text: str) -> str:
        """
        è¯„ä¼°å†…å®¹è´¨é‡
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            
        Returns:
            str: è´¨é‡è¯„ä¼°
        """
        if len(text) < 50:
            return "ç®€çŸ­"
        elif len(text) < 200:
            return "é€‚ä¸­"
        elif len(text) < 500:
            return "è¯¦ç»†"
        else:
            return "å…¨é¢"

    def _clean_and_enhance_text(self, text: str) -> str:
        """
        æ¸…ç†å¹¶å¢å¼ºæ–‡æœ¬å†…å®¹ - é’ˆå¯¹å­¦æœ¯æ–‡æœ¬ä¼˜åŒ–
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            
        Returns:
            str: æ¸…ç†åçš„æ–‡æœ¬
        """
        if not text:
            return ""
        
        # ç§»é™¤å¸¸è§çš„PDFä¹±ç å’Œæ— ç”¨å­—ç¬¦
        garbage_patterns = [
            r'\[fOMN-_Ãƒ\[fOMÂºe\(ÃvÃ‘mK\^sSÃ°\s*\d+\s*\d+',  # ç‰¹å®šä¹±ç æ¨¡å¼
            r'[^\x00-\x7F\u4e00-\u9fff\u3000-\u303f\uff00-\uffef\s.,ï¼Œã€‚ã€ï¼›ï¼š""''ï¼ˆï¼‰()[\]ã€ã€‘<>ã€Šã€‹-]+',  # ä¿ç•™åŸºæœ¬å­—ç¬¦
            r'(\d+)\s*\1\s*\1',  # é‡å¤æ•°å­—æ¨¡å¼
            r'\.{4,}',  # å¤šä¸ªè¿ç»­çš„ç‚¹å·
            r'\s+\.{2,}\s+',  # ç©ºæ ¼åŒ…å›´çš„å¤šä¸ªç‚¹
        ]
        
        cleaned_text = text
        for pattern in garbage_patterns:
            cleaned_text = re.sub(pattern, ' ', cleaned_text)
        
        # æ ‡å‡†åŒ–ç©ºç™½å­—ç¬¦å’Œæ ‡ç‚¹
        cleaned_text = re.sub(r'\s+', ' ', cleaned_text)
        cleaned_text = re.sub(r'([ã€‚ï¼ï¼Ÿ])\s*([a-zA-Z])', r'\1 \2', cleaned_text)  # ä¸­è‹±æ–‡é—´åŠ ç©ºæ ¼
        cleaned_text = cleaned_text.strip()
        
        # å¦‚æœæ¸…ç†åæ–‡æœ¬è´¨é‡å¤ªä½ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²
        if len(cleaned_text) < 20:
            return ""
        
        # è®¡ç®—æœ‰æ„ä¹‰å­—ç¬¦æ¯”ä¾‹
        meaningful_chars = len(re.findall(r'[\w\u4e00-\u9fff]', cleaned_text))
        total_chars = len(cleaned_text)
        
        if total_chars > 0 and meaningful_chars / total_chars < 0.4:  # è°ƒæ•´ä¸º40%
            logger.debug(f"ä¸¢å¼ƒä½è´¨é‡æ–‡æœ¬å—ï¼Œæœ‰æ„ä¹‰å­—ç¬¦æ¯”ä¾‹: {meaningful_chars/total_chars:.2f}")
            return ""
        
        # æ™ºèƒ½æˆªæ–­ï¼šä¿ç•™å®Œæ•´å¥å­
        max_chunk_length = getattr(Config, 'MAX_CHUNK_LENGTH_FOR_PROMPT', 800)
        if len(cleaned_text) > max_chunk_length:
            # ä¼˜å…ˆä¿ç•™å¥å­å®Œæ•´æ€§
            sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', cleaned_text[:max_chunk_length])
            if len(sentences) > 1:
                cleaned_text = 'ã€‚'.join(sentences[:-1]) + "ã€‚"
            else:
                cleaned_text = cleaned_text[:max_chunk_length] + "..."
        
        return cleaned_text

    def _format_conversation_history(self, history: List[Dict]) -> str:
        """
        æ ¼å¼åŒ–å¯¹è¯å†å² - å¢å¼ºç‰ˆ
        
        Args:
            history: å¯¹è¯å†å²åˆ—è¡¨
            
        Returns:
            str: æ ¼å¼åŒ–çš„å¯¹è¯å†å²
        """
        if not history:
            return ""
        
        history_parts = ["## ğŸ’¬ å¯¹è¯å†å²"]
        history_parts.append("\nä»¥ä¸‹æ˜¯æ‚¨ä¸æˆ‘ä¹‹å‰çš„å¯¹è¯ï¼Œä¾›å‚è€ƒï¼š\n")
        
        # åªä¿ç•™æœ€è¿‘å‡ è½®å¯¹è¯ï¼Œé¿å…è¿‡é•¿
        recent_history = history[-Config.RAG_CONVERSATION_MAX_TURNS:]
        logger.debug(f"Formatting {len(recent_history)} turns of conversation history.")
        
        for i, turn in enumerate(recent_history, 1):
            role = turn.get('role', 'unknown')
            content = turn.get('content', '').strip()
            
            if content:
                if role == 'user':
                    history_parts.append(f"**ğŸ‘¤ æ‚¨çš„é—®é¢˜ {i}**ï¼š{content}")
                elif role == 'assistant':
                    # ç®€åŒ–AIå›ç­”ï¼Œåªä¿ç•™æ ¸å¿ƒå†…å®¹
                    simplified_content = self._simplify_ai_response(content)
                    history_parts.append(f"**ğŸ¤– æˆ‘çš„å›ç­” {i}**ï¼š{simplified_content}")
                    
                history_parts.append("")  # ç©ºè¡Œåˆ†éš”
        
        return "\n".join(history_parts)

    def _simplify_ai_response(self, response: str) -> str:
        """
        ç®€åŒ–AIå›ç­”ï¼Œæå–å…³é”®ä¿¡æ¯ - å¢å¼ºç‰ˆ
        
        Args:
            response: åŸå§‹AIå›ç­”
            
        Returns:
            str: ç®€åŒ–åçš„å›ç­”
        """
        # æå–ä¸»è¦å›ç­”éƒ¨åˆ†
        main_answer_patterns = [
            r'\*\*ä¸»è¦å›ç­”\*\*[ï¼š:]?\s*(.+?)(?=\n\*\*|$)',
            r'(?:^|\n)([^*\n][^*\n]*?)(?=\n\*\*|$)',
        ]
        
        for pattern in main_answer_patterns:
            match = re.search(pattern, response, re.DOTALL | re.IGNORECASE)
            if match:
                answer = match.group(1).strip()
                
                # æ¸…ç†ç­”æ¡ˆå†…å®¹
                answer = re.sub(r'ã€æ¥æº\d+ã€‘', '', answer)  # ç§»é™¤æ¥æºæ ‡è®°
                answer = re.sub(r'\s+', ' ', answer).strip()  # æ ‡å‡†åŒ–ç©ºç™½
                
                # é™åˆ¶é•¿åº¦
                if len(answer) > 150:
                    # å°è¯•åœ¨å¥å·å¤„æˆªæ–­
                    sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', answer[:150])
                    if len(sentences) > 1:
                        answer = 'ã€‚'.join(sentences[:-1]) + "ã€‚"
                    else:
                        answer = answer[:150] + "..."
                
                logger.debug(f"Simplified AI response to: {answer}")
                return answer
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ¼å¼åŒ–çš„ç­”æ¡ˆï¼Œç›´æ¥æˆªå–å‰150å­—ç¬¦
        simplified = response[:150] + "..." if len(response) > 150 else response
        simplified = re.sub(r'ã€æ¥æº\d+ã€‘', '', simplified)
        simplified = re.sub(r'\s+', ' ', simplified).strip()
        
        return simplified

    def _optimize_prompt_length(self, prompt: str) -> str:
        """
        ä¼˜åŒ–æç¤ºè¯é•¿åº¦ï¼Œç¡®ä¿ä¸è¶…è¿‡tokené™åˆ¶
        
        Args:
            prompt: åŸå§‹æç¤ºè¯
            
        Returns:
            str: ä¼˜åŒ–åçš„æç¤ºè¯
        """
        estimated_tokens = self._estimate_tokens(prompt)
        logger.debug(f"Estimated tokens for prompt: {estimated_tokens}. Max allowed context tokens: {self.max_context_tokens}")

        if estimated_tokens <= self.max_context_tokens:
            return prompt
        
        logger.warning(f"Prompt is too long ({estimated_tokens} tokens). Attempting to compress.")
        # å¦‚æœè¶…é•¿ï¼Œéœ€è¦å‹ç¼©ä¸Šä¸‹æ–‡éƒ¨åˆ†
        compressed_prompt = self._compress_prompt(prompt, estimated_tokens)
        final_estimated_tokens = self._estimate_tokens(compressed_prompt)
        logger.info(f"Compressed prompt. Original tokens: {estimated_tokens}, New tokens: {final_estimated_tokens}")
        return compressed_prompt

    def _estimate_tokens(self, text: str) -> int:
        """
        ä¼°ç®—æ–‡æœ¬çš„tokenæ•°é‡ - æ”¹è¿›ç‰ˆ
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            int: ä¼°ç®—çš„tokenæ•°é‡
        """
        # ä¸­æ–‡å­—ç¬¦æ•°
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        
        # è‹±æ–‡å•è¯æ•°
        english_words = len(re.findall(r'\b[a-zA-Z0-9]+\b', text))
        
        # ç‰¹æ®Šç¬¦å·å’Œæ ‡ç‚¹
        special_chars = len(re.findall(r'[^\w\s\u4e00-\u9fff]', text))
        
        # åŸºäºå®é™…ç»éªŒçš„tokenä¼°ç®—
        # ä¸­æ–‡ï¼š1.3ä¸ªå­—ç¬¦â‰ˆ1ä¸ªtoken
        # è‹±æ–‡ï¼š4ä¸ªå­—ç¬¦â‰ˆ1ä¸ªtoken
        # ç¬¦å·ï¼šæŒ‰0.5è®¡ç®—
        estimated_tokens = int(chinese_chars * 1.3 + english_words * 1.0 + special_chars * 0.5)
        
        return estimated_tokens

    def _compress_prompt(self, prompt: str, current_tokens: int) -> str:
        """
        å‹ç¼©æç¤ºè¯ï¼Œä¸»è¦é€šè¿‡æˆªæ–­ä¸Šä¸‹æ–‡æ¥å®ç°
        
        Args:
            prompt: åŸå§‹æç¤ºè¯
            current_tokens: å½“å‰ä¼°ç®—çš„tokenæ•°é‡
            
        Returns:
            str: å‹ç¼©åçš„æç¤ºè¯
        """
        # åˆ†è§£æç¤ºè¯çš„å„ä¸ªéƒ¨åˆ†
        parts = prompt.split("\n\n")
        
        # æ‰¾åˆ°æ–‡çŒ®å†…å®¹éƒ¨åˆ†
        context_start = -1
        context_end = -1
        for i, part in enumerate(parts):
            if "ğŸ“š ç›¸å…³æ–‡çŒ®å†…å®¹" in part:
                context_start = i
            elif context_start != -1 and ("ğŸ¤” ç”¨æˆ·é—®é¢˜" in part or "ğŸ’¬ å¯¹è¯å†å²" in part or "é‡è¦æŒ‡ç¤º" in part):
                context_end = i
                break
        
        if context_start == -1:
            logger.error("Could not find context section to compress")
            return prompt
        
        if context_end == -1:
            context_end = len(parts)
        
        # è®¡ç®—éä¸Šä¸‹æ–‡éƒ¨åˆ†çš„token
        non_context_parts = parts[:context_start] + parts[context_end:]
        non_context_text = "\n\n".join(non_context_parts)
        non_context_tokens = self._estimate_tokens(non_context_text)
        
        # è®¡ç®—ä¸Šä¸‹æ–‡éƒ¨åˆ†å…è®¸çš„æœ€å¤§token
        allowed_context_tokens = self.max_context_tokens - non_context_tokens - 200  # ç•™200tokenç¼“å†²
        
        if allowed_context_tokens <= 0:
            logger.error("Not enough tokens for context after reserving for other parts")
            return "\n\n".join(parts[:context_start] + ["## ğŸ“š ç›¸å…³æ–‡çŒ®å†…å®¹\nâš ï¸ ä¸Šä¸‹æ–‡å†…å®¹å› é•¿åº¦é™åˆ¶å·²è¢«ç§»é™¤"] + parts[context_end:])
        
        # å‹ç¼©ä¸Šä¸‹æ–‡éƒ¨åˆ†
        context_parts = parts[context_start:context_end]
        context_text = "\n\n".join(context_parts)
        
        # æŒ‰æ¥æºé€ä¸ªæ·»åŠ ï¼Œç›´åˆ°è¾¾åˆ°tokené™åˆ¶
        compressed_context = parts[context_start]  # ä¿ç•™æ ‡é¢˜
        current_context_tokens = self._estimate_tokens(compressed_context)
        
        # æå–å„ä¸ªæ¥æº
        source_pattern = r'### ã€æ¥æº\d+ã€‘.*?(?=### ã€æ¥æº\d+ã€‘|---|\n\nğŸ’¡|$)'
        sources = re.findall(source_pattern, context_text, re.DOTALL)
        
        for source in sources:
            source_tokens = self._estimate_tokens(source)
            if current_context_tokens + source_tokens <= allowed_context_tokens:
                compressed_context += "\n\n" + source
                current_context_tokens += source_tokens
            else:
                logger.info(f"Context truncation: Stopped adding sources. Used {current_context_tokens} of {allowed_context_tokens} allowed tokens")
                break
        
        # é‡æ–°ç»„è£…æç¤ºè¯
        final_parts = parts[:context_start] + [compressed_context] + parts[context_end:]
        return "\n\n".join(final_parts)

    def build_preset_questions_prompt(self, literature_title: str, literature_summary: str = "") -> List[str]:
        """
        æ„å»ºé¢„è®¾é—®é¢˜åˆ—è¡¨
        
        Args:
            literature_title: æ–‡çŒ®æ ‡é¢˜
            literature_summary: æ–‡çŒ®æ‘˜è¦ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            List[str]: é¢„è®¾é—®é¢˜åˆ—è¡¨
        """
        base_questions = [
            "è¿™ç¯‡æ–‡çŒ®çš„ä¸»è¦è®ºç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ",
            "æ–‡çŒ®ä¸­ä½¿ç”¨äº†å“ªäº›ç ”ç©¶æ–¹æ³•ï¼Ÿ",
            "è¿™é¡¹ç ”ç©¶æœ‰ä»€ä¹ˆåˆ›æ–°ç‚¹å’Œè´¡çŒ®ï¼Ÿ",
            "ç ”ç©¶å­˜åœ¨å“ªäº›å±€é™æ€§ï¼Ÿ",
            "ä¸»è¦ç»“è®ºå’Œå‘ç°æ˜¯ä»€ä¹ˆï¼Ÿ",
            "æ–‡çŒ®çš„ç†è®ºæ¡†æ¶æ˜¯ä»€ä¹ˆï¼Ÿ",
            "ç ”ç©¶çš„å®é™…åº”ç”¨ä»·å€¼å¦‚ä½•ï¼Ÿ",
            "æ–‡çŒ®ä¸­æåˆ°äº†å“ªäº›æœªæ¥ç ”ç©¶æ–¹å‘ï¼Ÿ"
        ]
        
        # æ ¹æ®æ–‡çŒ®æ ‡é¢˜ç”Ÿæˆä¸ªæ€§åŒ–é—®é¢˜
        if literature_title:
            title_based_questions = [
                f"è¯·è§£é‡Šã€Š{literature_title}ã€‹è¿™ç¯‡æ–‡çŒ®çš„æ ¸å¿ƒå†…å®¹",
                f"ã€Š{literature_title}ã€‹ä¸å…¶ä»–ç›¸å…³ç ”ç©¶æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ"
            ]
            base_questions.extend(title_based_questions)
        
        return base_questions[:8]  # è¿”å›æœ€å¤š8ä¸ªé—®é¢˜

    def build_error_response_prompt(self, error_type: str, question: str) -> str:
        """
        æ„å»ºé”™è¯¯å“åº”æç¤ºè¯ - å¢å¼ºç‰ˆ
        
        Args:
            error_type: é”™è¯¯ç±»å‹
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            str: é”™è¯¯å“åº”å†…å®¹
        """
        error_responses = {
            "no_content": f"""
æŠ±æ­‰ï¼Œæˆ‘åœ¨å½“å‰æ–‡çŒ®ä¸­æ²¡æœ‰æ‰¾åˆ°ä¸æ‚¨çš„é—®é¢˜ã€Œ{question}ã€ç›¸å…³çš„å†…å®¹ã€‚

ğŸ” **å»ºè®®å°è¯•**ï¼š
1. ä½¿ç”¨ä¸åŒçš„å…³é”®è¯é‡æ–°è¡¨è¿°é—®é¢˜
2. ç¡®è®¤é—®é¢˜æ˜¯å¦ä¸å½“å‰æ–‡çŒ®çš„ä¸»é¢˜ç›¸å…³
3. æŸ¥çœ‹æˆ‘æä¾›çš„é¢„è®¾é—®é¢˜è·å–çµæ„Ÿ
4. å°è¯•æ›´å…·ä½“æˆ–æ›´å®½æ³›çš„é—®é¢˜è¡¨è¿°

ğŸ’¡ **æç¤º**ï¼šæ‚¨å¯ä»¥è¯¢é—®æ–‡çŒ®çš„ä¸»è¦å†…å®¹ã€ç ”ç©¶æ–¹æ³•ã€ä¸»è¦å‘ç°ç­‰æ ¸å¿ƒé—®é¢˜ã€‚
            """,
            
            "api_error": f"""
æŠ±æ­‰ï¼ŒAIæœåŠ¡æš‚æ—¶é‡åˆ°æŠ€æœ¯é—®é¢˜ï¼Œæ— æ³•å¤„ç†æ‚¨çš„é—®é¢˜ï¼šã€Œ{question}ã€

ğŸ”§ **è¯·ç¨åé‡è¯•**ï¼š
- ç³»ç»Ÿæ­£åœ¨è‡ªåŠ¨æ¢å¤ä¸­
- é€šå¸¸å‡ åˆ†é’Ÿå†…å³å¯æ¢å¤æ­£å¸¸
- æ‚¨å¯ä»¥å…ˆæŸ¥çœ‹é¢„è®¾é—®é¢˜æˆ–æµè§ˆæ–‡çŒ®å†…å®¹

å¦‚æœé—®é¢˜æŒç»­ï¼Œè¯·è”ç³»æŠ€æœ¯æ”¯æŒã€‚
            """,
            
            "context_too_long": f"""
æ‚¨çš„é—®é¢˜ã€Œ{question}ã€æ¶‰åŠçš„å†…å®¹è¾ƒä¸ºå¹¿æ³›ï¼Œå»ºè®®å°†å…¶åˆ†è§£ä¸ºæ›´å…·ä½“çš„å­é—®é¢˜ã€‚

ğŸ“ **å»ºè®®åˆ†è§£æ–¹å¼**ï¼š
1. å°†å¤åˆé—®é¢˜æ‹†åˆ†ä¸ºå•ä¸ªæ¦‚å¿µ
2. é’ˆå¯¹ç‰¹å®šçš„ç ”ç©¶æ–¹é¢æé—®
3. é€æ­¥æ·±å…¥ï¼Œä»ä¸€èˆ¬åˆ°å…·ä½“

ğŸ’¡ **ç¤ºä¾‹**ï¼šå¦‚æœé—®é¢˜æ¶‰åŠå¤šä¸ªç†è®ºï¼Œå¯ä»¥åˆ†åˆ«è¯¢é—®æ¯ä¸ªç†è®ºçš„å†…å®¹ã€‚
            """,
            
            "low_confidence": f"""
åŸºäºå½“å‰æ–‡çŒ®å†…å®¹ï¼Œæˆ‘å¯¹é—®é¢˜ã€Œ{question}ã€çš„å›ç­”æŠŠæ¡åº¦è¾ƒä½ã€‚

âš ï¸ **å¯èƒ½åŸå› **ï¼š
- æ–‡çŒ®ä¸­ç›¸å…³ä¿¡æ¯æœ‰é™
- é—®é¢˜è¶…å‡ºäº†æ–‡çŒ®çš„ç ”ç©¶èŒƒå›´
- éœ€è¦æ›´å¤šèƒŒæ™¯ä¿¡æ¯æ‰èƒ½å‡†ç¡®å›ç­”

ğŸ¯ **å»ºè®®**ï¼š
1. å°è¯•è¯¢é—®æ–‡çŒ®ä¸­æ˜ç¡®è®¨è®ºçš„ä¸»é¢˜
2. å‚è€ƒé¢„è®¾é—®é¢˜è·å–æ€è·¯
3. æŸ¥é˜…æ›´å¤šç›¸å…³æ–‡çŒ®è·å¾—å…¨é¢ç†è§£
            """
        }
        
        return error_responses.get(error_type, f"å¤„ç†é—®é¢˜ã€Œ{question}ã€æ—¶å‡ºç°æœªçŸ¥é”™è¯¯ï¼Œè¯·é‡è¯•ã€‚")

    def validate_prompt_quality(self, prompt: str) -> Dict[str, Any]:
        """
        éªŒè¯æç¤ºè¯è´¨é‡ - åŸºäºæœ€ä½³å®è·µ
        
        Args:
            prompt: æç¤ºè¯å†…å®¹
            
        Returns:
            Dict: è´¨é‡è¯„ä¼°ç»“æœ
        """
        validation_result = {
            "is_valid": True,
            "issues": [],
            "token_count": self._estimate_tokens(prompt),
            "has_context": "ğŸ“š ç›¸å…³æ–‡çŒ®å†…å®¹" in prompt,
            "has_role_definition": "ä¸“ä¸šæ–‡çŒ®ç ”ç©¶åŠ©æ‰‹" in prompt,
            "has_capability_list": "æ ¸å¿ƒèƒ½åŠ›" in prompt,
            "has_limitation_list": "ä¸¥æ ¼é™åˆ¶" in prompt,
            "has_format_specification": "å“åº”æ ¼å¼åè®®" in prompt,
            "has_citation_guidance": "å¼•ç”¨æ ‡æ³¨è§„èŒƒ" in prompt,
            "quality_score": 0.0
        }
        
        # æ£€æŸ¥é•¿åº¦
        if validation_result["token_count"] > self.max_context_tokens:
            validation_result["issues"].append("æç¤ºè¯è¿‡é•¿")
            validation_result["is_valid"] = False
        
        # æ£€æŸ¥å¿…è¦ç»„ä»¶
        required_components = [
            ("has_context", "ç¼ºå°‘æ–‡çŒ®å†…å®¹"),
            ("has_role_definition", "ç¼ºå°‘è§’è‰²å®šä¹‰"),
            ("has_capability_list", "ç¼ºå°‘èƒ½åŠ›è¯´æ˜"),
            ("has_limitation_list", "ç¼ºå°‘é™åˆ¶è¯´æ˜"),
            ("has_format_specification", "ç¼ºå°‘æ ¼å¼è§„èŒƒ"),
            ("has_citation_guidance", "ç¼ºå°‘å¼•ç”¨æŒ‡å¯¼")
        ]
        
        valid_components = 0
        for component, error_msg in required_components:
            if validation_result[component]:
                valid_components += 1
            else:
                validation_result["issues"].append(error_msg)
        
        # è®¡ç®—è´¨é‡åˆ†æ•°
        validation_result["quality_score"] = valid_components / len(required_components)
        
        # å¦‚æœè´¨é‡åˆ†æ•°ä½äº0.8ï¼Œæ ‡è®°ä¸ºæ— æ•ˆ
        if validation_result["quality_score"] < 0.8:
            validation_result["is_valid"] = False
        
        return validation_result 